{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 2: No Positional Awareness\n",
        "\n",
        "Demonstrates why transformers need positional encodings.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/02_positional_awareness/demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch matplotlib numpy -q\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Self-attention without positional encoding\n",
        "def attention(x, W_q, W_k, W_v):\n",
        "    Q, K, V = x @ W_q, x @ W_k, x @ W_v\n",
        "    scores = (Q @ K.T) / np.sqrt(K.shape[-1])\n",
        "    return F.softmax(scores, dim=-1) @ V\n",
        "\n",
        "# \"Dog bites man\" vs \"man bites Dog\"\n",
        "torch.manual_seed(42)\n",
        "dog, bites, man = torch.randn(8), torch.randn(8), torch.randn(8)\n",
        "W_q = W_k = W_v = torch.randn(8, 8) * 0.1\n",
        "\n",
        "x1 = torch.stack([dog, bites, man])  # Dog bites man\n",
        "x2 = torch.stack([man, bites, dog])  # man bites Dog\n",
        "\n",
        "out1, out2 = attention(x1, W_q, W_k, W_v), attention(x2, W_q, W_k, W_v)\n",
        "\n",
        "print(\"Without positional encoding:\")\n",
        "print(f\"  'Dog bites man' output[0] ≈ 'man bites Dog' output[2]: {torch.allclose(out1[0], out2[2], atol=1e-4)}\")\n",
        "print(\"\\n⚠️ The model CAN'T distinguish word order!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution: Sinusoidal Positional Encoding\n",
        "def sinusoidal_pe(max_len, d_model):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(max_len).unsqueeze(1).float()\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "pe = sinusoidal_pe(50, 64)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.imshow(pe.T, cmap='RdBu', aspect='auto')\n",
        "plt.xlabel('Position'); plt.ylabel('Dimension')\n",
        "plt.title('Sinusoidal Positional Encoding - Each position has unique pattern!')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Now each position has a unique encoding that tells the model about order!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
