{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸŽ­ Performer: Fast Attention via FAVOR+\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/07_performer/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovation\n", "- **FAVOR+**: Fast Attention Via positive Orthogonal Random features\n", "- **O(N) Complexity**: Linear instead of quadratic!\n", "- **Unbiased Estimator**: Approximates softmax attention"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## The Math Behind FAVOR+\n", "\n", "Standard attention: `Attn = softmax(QK^T / âˆšd) V`\n", "\n", "FAVOR+ approximation:\n", "1. `K(x,y) â‰ˆ Ï†(x)^T Ï†(y)` where Ï† is random feature map\n", "2. `Attn â‰ˆ Ï†(Q) (Ï†(K)^T V)` - can compute in O(N)!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_favor_math():\n", "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n", "    \n", "    # Standard attention computation\n", "    ax = axes[0]\n", "    N, d = 8, 4\n", "    Q = np.random.randn(N, d)\n", "    K = np.random.randn(N, d)\n", "    V = np.random.randn(N, d)\n", "    \n", "    # QK^T is NÃ—N\n", "    ax.text(0.5, 0.9, 'Standard Attention', fontsize=14, ha='center', transform=ax.transAxes)\n", "    ax.text(0.5, 0.7, r'$\\mathbf{QK}^T$ â†’ NÃ—N matrix', fontsize=11, ha='center', transform=ax.transAxes)\n", "    ax.text(0.5, 0.5, f'For N={N}, d={d}:', fontsize=10, ha='center', transform=ax.transAxes)\n", "    ax.text(0.5, 0.3, f'Memory: O(NÂ²) = O({N**2})', fontsize=10, ha='center', transform=ax.transAxes, color='red')\n", "    ax.axis('off')\n", "    \n", "    # FAVOR+ decomposition\n", "    ax = axes[1]\n", "    ax.text(0.5, 0.9, 'FAVOR+ Trick', fontsize=14, ha='center', transform=ax.transAxes)\n", "    ax.text(0.5, 0.7, r'$\\phi(Q) [\\phi(K)^T V]$', fontsize=11, ha='center', transform=ax.transAxes)\n", "    ax.text(0.5, 0.5, 'Compute (K^T V) first!', fontsize=10, ha='center', transform=ax.transAxes)\n", "    ax.text(0.5, 0.3, f'Memory: O(dÂ²) = O({d**2})', fontsize=10, ha='center', transform=ax.transAxes, color='green')\n", "    ax.axis('off')\n", "    \n", "    # Comparison\n", "    ax = axes[2]\n", "    seq_lens = [64, 256, 1024, 4096]\n", "    standard = [n**2 for n in seq_lens]\n", "    favor = [n * d**2 for n in seq_lens]\n", "    \n", "    x = np.arange(len(seq_lens))\n", "    width = 0.35\n", "    ax.bar(x - width/2, standard, width, label='Standard O(NÂ²)', color='coral')\n", "    ax.bar(x + width/2, favor, width, label='FAVOR+ O(NdÂ²)', color='lightgreen')\n", "    ax.set_xticks(x)\n", "    ax.set_xticklabels(seq_lens)\n", "    ax.set_xlabel('Sequence Length')\n", "    ax.set_ylabel('Operations')\n", "    ax.legend()\n", "    ax.set_yscale('log')\n", "    ax.set_title('Complexity Comparison')\n", "    ax.grid(True, alpha=0.3)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "visualize_favor_math()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Random Feature Maps"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def random_feature_map(x, n_features, is_query=True):\n", "    \"\"\"FAVOR+ random feature map using positive random features.\"\"\"\n", "    d = x.shape[-1]\n", "    \n", "    # Orthogonal random features (simplified)\n", "    torch.manual_seed(0)  # Fixed for reproducibility\n", "    omega = torch.randn(d, n_features, device=x.device) / math.sqrt(d)\n", "    \n", "    # Project: x @ omega â†’ (B, N, n_features)\n", "    projection = x @ omega\n", "    \n", "    # Apply nonlinearity: exp(x - ||x||Â²/2) for positive features\n", "    norm_sq = (x ** 2).sum(dim=-1, keepdim=True) / 2\n", "    \n", "    # Positive random features\n", "    features = torch.exp(projection - norm_sq) / math.sqrt(n_features)\n", "    \n", "    return features\n", "\n", "# Demonstrate approximation quality\n", "def compare_attention_methods(seq_len=64, d_model=32, n_features=64):\n", "    Q = torch.randn(1, seq_len, d_model)\n", "    K = torch.randn(1, seq_len, d_model)\n", "    V = torch.randn(1, seq_len, d_model)\n", "    \n", "    # Standard attention\n", "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_model)\n", "    attn_standard = F.softmax(scores, dim=-1)\n", "    out_standard = attn_standard @ V\n", "    \n", "    # FAVOR+ approximation\n", "    Q_prime = random_feature_map(Q / math.sqrt(math.sqrt(d_model)), n_features)\n", "    K_prime = random_feature_map(K / math.sqrt(math.sqrt(d_model)), n_features)\n", "    \n", "    # Linear attention: Q'(K'V) instead of (Q'K')V\n", "    KV = K_prime.transpose(-2, -1) @ V  # (n_features, d_model)\n", "    out_favor = Q_prime @ KV  # (seq_len, d_model)\n", "    \n", "    # Normalize\n", "    normalizer = Q_prime @ K_prime.sum(dim=1, keepdim=True).transpose(-2, -1)\n", "    out_favor = out_favor / (normalizer + 1e-6)\n", "    \n", "    # Compare\n", "    mse = F.mse_loss(out_standard, out_favor).item()\n", "    cosine = F.cosine_similarity(out_standard.flatten(), out_favor.flatten(), dim=0).item()\n", "    \n", "    return mse, cosine\n", "\n", "# Test with different number of random features\n", "n_features_list = [16, 32, 64, 128, 256]\n", "mses, cosines = [], []\n", "\n", "for n_feat in n_features_list:\n", "    mse, cos = compare_attention_methods(n_features=n_feat)\n", "    mses.append(mse)\n", "    cosines.append(cos)\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n", "axes[0].plot(n_features_list, mses, 'ro-')\n", "axes[0].set_xlabel('Number of Random Features')\n", "axes[0].set_ylabel('MSE')\n", "axes[0].set_title('Approximation Error')\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "axes[1].plot(n_features_list, cosines, 'go-')\n", "axes[1].set_xlabel('Number of Random Features')\n", "axes[1].set_ylabel('Cosine Similarity')\n", "axes[1].set_title('Output Similarity')\n", "axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('More random features â†’ better approximation!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Performer Implementation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PerformerAttention(nn.Module):\n", "    \"\"\"FAVOR+ attention with linear complexity.\"\"\"\n", "    def __init__(self, d_model, n_heads, n_features=None, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.n_features = n_features or self.d_k\n", "        \n", "        self.W_qkv = nn.Linear(d_model, 3 * d_model)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        # Random projection matrix (fixed)\n", "        self.register_buffer('omega', torch.randn(self.d_k, self.n_features) / math.sqrt(self.d_k))\n", "    \n", "    def _feature_map(self, x):\n", "        \"\"\"Positive random feature map.\"\"\"\n", "        projection = x @ self.omega\n", "        norm_sq = (x ** 2).sum(dim=-1, keepdim=True) / 2\n", "        return torch.exp(projection - norm_sq) / math.sqrt(self.n_features)\n", "    \n", "    def forward(self, x, causal=True):\n", "        B, T, C = x.shape\n", "        \n", "        # QKV projection\n", "        qkv = self.W_qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n", "        Q, K, V = qkv[0], qkv[1], qkv[2]  # (B, H, T, d_k)\n", "        \n", "        # Apply feature map\n", "        Q_prime = self._feature_map(Q / math.sqrt(math.sqrt(self.d_k)))\n", "        K_prime = self._feature_map(K / math.sqrt(math.sqrt(self.d_k)))\n", "        \n", "        if causal:\n", "            # Causal linear attention using prefix sums\n", "            # Compute cumulative sum: KV[i] = sum(K'[j] * V[j]) for j <= i\n", "            KV = torch.zeros(B, self.n_heads, self.n_features, self.d_k, device=x.device)\n", "            K_sum = torch.zeros(B, self.n_heads, self.n_features, 1, device=x.device)\n", "            \n", "            outputs = []\n", "            for t in range(T):\n", "                k_t = K_prime[:, :, t:t+1, :]  # (B, H, 1, n_feat)\n", "                v_t = V[:, :, t:t+1, :]  # (B, H, 1, d_k)\n", "                q_t = Q_prime[:, :, t:t+1, :]  # (B, H, 1, n_feat)\n", "                \n", "                KV = KV + k_t.transpose(-2, -1) @ v_t  # (B, H, n_feat, d_k)\n", "                K_sum = K_sum + k_t.transpose(-2, -1)  # (B, H, n_feat, 1)\n", "                \n", "                out_t = q_t @ KV / (q_t @ K_sum + 1e-6)\n", "                outputs.append(out_t)\n", "            \n", "            out = torch.cat(outputs, dim=2)  # (B, H, T, d_k)\n", "        else:\n", "            # Non-causal: Q'(K'V) - much faster\n", "            KV = K_prime.transpose(-2, -1) @ V  # (B, H, n_feat, d_k)\n", "            K_sum = K_prime.sum(dim=2, keepdim=True).transpose(-2, -1)  # (B, H, n_feat, 1)\n", "            out = Q_prime @ KV / (Q_prime @ K_sum + 1e-6)\n", "        \n", "        out = out.transpose(1, 2).reshape(B, T, C)\n", "        return self.W_o(out)\n", "\n", "class PerformerBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, n_features=None, dropout=0.1):\n", "        super().__init__()\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.attn = PerformerAttention(d_model, n_heads, n_features, dropout)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.Linear(d_model, 4 * d_model),\n", "            nn.GELU(),\n", "            nn.Linear(4 * d_model, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        x = x + self.attn(self.norm1(x))\n", "        x = x + self.ff(self.norm2(x))\n", "        return x\n", "\n", "class Performer(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, n_features=64, max_len=512, dropout=0.1):\n", "        super().__init__()\n", "        self.embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = nn.Embedding(max_len, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        self.layers = nn.ModuleList([PerformerBlock(d_model, n_heads, n_features, dropout) for _ in range(n_layers)])\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size)\n", "    \n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n", "        \n", "        x = self.dropout(self.embed(x) + self.pos_embed(pos))\n", "        for layer in self.layers:\n", "            x = layer(x)\n", "        \n", "        return self.head(self.norm(x))\n", "\n", "model = Performer(vocab_size=1000, d_model=64, n_heads=4, n_layers=2, n_features=32).to(device)\n", "print(f'Performer Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training Performer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Dataset\n", "text = 'the quick brown fox jumps over the lazy dog ' * 300\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n", "\n", "# Training\n", "seq_len = 64\n", "model = Performer(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, n_features=32, max_len=seq_len).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "\n", "losses = []\n", "n_steps = 300\n", "\n", "print('Training Performer with linear attention...')\n", "for step in range(n_steps):\n", "    idx = torch.randint(0, len(data) - seq_len - 1, (16,))\n", "    x = torch.stack([data[i:i+seq_len] for i in idx]).to(device)\n", "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx]).to(device)\n", "    \n", "    optimizer.zero_grad()\n", "    logits = model(x)\n", "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    if (step + 1) % 50 == 0:\n", "        print(f'Step {step+1}: Loss = {loss.item():.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Step')\n", "plt.ylabel('Loss')\n", "plt.title('Performer Training (Linear Attention)')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. FAVOR+ approximates softmax via random features')\n", "print('2. O(N) complexity instead of O(NÂ²)')\n", "print('3. More random features = better approximation')\n", "print('4. Great for very long sequences (10K+ tokens)')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
