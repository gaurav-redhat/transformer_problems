{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üìù GPT: Generative Pre-trained Transformer\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/transformer/transformer_architectures/03_gpt/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Features\n", "- **Decoder-only**: Autoregressive generation\n", "- **Causal mask**: Only see past tokens\n", "- **Next token prediction**: P(next | previous)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## GPT Architecture"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CausalSelfAttention(nn.Module):\n", "    def __init__(self, d_model, n_heads, max_len=512, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.d_model = d_model\n", "        \n", "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n", "        self.c_proj = nn.Linear(d_model, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        # Causal mask (lower triangular)\n", "        self.register_buffer('mask', torch.tril(torch.ones(max_len, max_len)).view(1, 1, max_len, max_len))\n", "    \n", "    def forward(self, x):\n", "        B, T, C = x.shape\n", "        \n", "        # Q, K, V projection\n", "        qkv = self.c_attn(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n", "        Q, K, V = qkv[0], qkv[1], qkv[2]\n", "        \n", "        # Causal attention\n", "        attn = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n", "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n", "        attn = F.softmax(attn, dim=-1)\n", "        attn = self.dropout(attn)\n", "        \n", "        out = (attn @ V).transpose(1, 2).reshape(B, T, C)\n", "        return self.c_proj(out), attn\n", "\n", "class GPTBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, dropout=0.1):\n", "        super().__init__()\n", "        self.ln1 = nn.LayerNorm(d_model)\n", "        self.attn = CausalSelfAttention(d_model, n_heads, dropout=dropout)\n", "        self.ln2 = nn.LayerNorm(d_model)\n", "        self.mlp = nn.Sequential(\n", "            nn.Linear(d_model, 4 * d_model),\n", "            nn.GELU(),\n", "            nn.Linear(4 * d_model, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        attn_out, _ = self.attn(self.ln1(x))\n", "        x = x + attn_out\n", "        x = x + self.mlp(self.ln2(x))\n", "        return x\n", "\n", "class GPT(nn.Module):\n", "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4, max_len=512, dropout=0.1):\n", "        super().__init__()\n", "        self.token_embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = nn.Embedding(max_len, d_model)\n", "        self.drop = nn.Dropout(dropout)\n", "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads, dropout) for _ in range(n_layers)])\n", "        self.ln_f = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n", "        \n", "        # Weight tying\n", "        self.token_embed.weight = self.head.weight\n", "    \n", "    def forward(self, idx):\n", "        B, T = idx.shape\n", "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n", "        \n", "        x = self.drop(self.token_embed(idx) + self.pos_embed(pos))\n", "        for block in self.blocks:\n", "            x = block(x)\n", "        x = self.ln_f(x)\n", "        return self.head(x)\n", "    \n", "    @torch.no_grad()\n", "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n", "        for _ in range(max_new_tokens):\n", "            idx_cond = idx[:, -512:]  # Crop to max_len\n", "            logits = self(idx_cond)[:, -1, :] / temperature\n", "            \n", "            if top_k is not None:\n", "                v, _ = torch.topk(logits, top_k)\n", "                logits[logits < v[:, [-1]]] = float('-inf')\n", "            \n", "            probs = F.softmax(logits, dim=-1)\n", "            next_idx = torch.multinomial(probs, num_samples=1)\n", "            idx = torch.cat([idx, next_idx], dim=1)\n", "        return idx\n", "\n", "model = GPT(vocab_size=5000, d_model=128, n_heads=4, n_layers=3).to(device)\n", "print(f'GPT Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Visualize Causal Mask"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize causal mask\n", "seq_len = 10\n", "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n", "\n", "plt.figure(figsize=(8, 6))\n", "plt.imshow(causal_mask, cmap='Greens')\n", "plt.xlabel('Key Position')\n", "plt.ylabel('Query Position')\n", "plt.title('Causal Attention Mask\\n(Green = Can Attend)')\n", "plt.colorbar()\n", "for i in range(seq_len):\n", "    for j in range(seq_len):\n", "        plt.text(j, i, int(causal_mask[i, j].item()), ha='center', va='center')\n", "plt.show()\n", "\n", "print('Each position can only attend to itself and previous positions.')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training: Character-Level Language Model"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create tiny character dataset\n", "text = '''\n", "the quick brown fox jumps over the lazy dog\n", "a fast red fox leaps across the sleeping hound\n", "quick foxes jump high over slow dogs\n", "the brown dog runs after the red fox\n", "lazy dogs sleep while foxes jump around\n", "''' * 100  # Repeat for more data\n", "\n", "# Character-level tokenization\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "idx_to_char = {i: c for i, c in enumerate(chars)}\n", "\n", "def encode(s):\n", "    return [char_to_idx[c] for c in s]\n", "\n", "def decode(l):\n", "    return ''.join([idx_to_char[i] for i in l])\n", "\n", "data = torch.tensor(encode(text), dtype=torch.long)\n", "print(f'Vocabulary size: {vocab_size}')\n", "print(f'Data length: {len(data)}')\n", "print(f'Characters: {\"\".join(chars)}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Data loader\n", "def get_batch(data, batch_size, block_size):\n", "    ix = torch.randint(len(data) - block_size, (batch_size,))\n", "    x = torch.stack([data[i:i+block_size] for i in ix])\n", "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n", "    return x.to(device), y.to(device)\n", "\n", "# Training\n", "model = GPT(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=3, max_len=64).to(device)\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n", "\n", "losses = []\n", "n_steps = 500\n", "batch_size = 32\n", "block_size = 32\n", "\n", "print('Training GPT (Character-Level LM)...')\n", "for step in range(n_steps):\n", "    x, y = get_batch(data, batch_size, block_size)\n", "    \n", "    optimizer.zero_grad()\n", "    logits = model(x)\n", "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    \n", "    if (step + 1) % 100 == 0:\n", "        print(f'Step {step+1}: Loss = {loss.item():.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Step')\n", "plt.ylabel('Loss')\n", "plt.title('GPT Training Loss')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generate text!\n", "model.eval()\n", "\n", "prompts = ['the ', 'fox ', 'dog ', 'quick ']\n", "\n", "print('Text Generation:')\n", "print('='*50)\n", "for prompt in prompts:\n", "    context = torch.tensor([encode(prompt)], dtype=torch.long).to(device)\n", "    generated = model.generate(context, max_new_tokens=50, temperature=0.8)\n", "    print(f'Prompt: \"{prompt}\"')\n", "    print(f'Generated: \"{decode(generated[0].tolist())}\"')\n", "    print()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize attention patterns\n", "model.eval()\n", "test_input = torch.tensor([encode('the fox')], dtype=torch.long).to(device)\n", "\n", "with torch.no_grad():\n", "    x = model.token_embed(test_input) + model.pos_embed(torch.arange(test_input.size(1), device=device))\n", "    _, attn = model.blocks[0].attn(model.blocks[0].ln1(x))\n", "\n", "plt.figure(figsize=(8, 6))\n", "plt.imshow(attn[0, 0].cpu(), cmap='Blues')\n", "plt.xlabel('Key Position')\n", "plt.ylabel('Query Position')\n", "plt.title('GPT Causal Attention (Head 0)')\n", "tokens = list('the fox')\n", "plt.xticks(range(len(tokens)), tokens)\n", "plt.yticks(range(len(tokens)), tokens)\n", "plt.colorbar()\n", "plt.show()\n", "\n", "print('\\nüéØ Key Takeaways:')\n", "print('1. GPT is decoder-only, autoregressive')\n", "print('2. Causal mask: Each token only sees past tokens')\n", "print('3. Next token prediction: Predict P(next | previous)')\n", "print('4. Great for text generation, completion, chatbots')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
