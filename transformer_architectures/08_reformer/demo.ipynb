{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ” Reformer: Efficient via LSH\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/08_reformer/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovations\n", "- **LSH Attention**: Locality Sensitive Hashing groups similar queries\n", "- **Reversible Layers**: Recompute activations, no memory storage\n", "- **Chunked FFN**: Process in chunks to save memory"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Locality Sensitive Hashing (LSH)\n", "\n", "Core idea: **Similar vectors should hash to the same bucket**\n", "\n", "For attention: Only compute attention within each bucket!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def angular_lsh(x, n_hashes, n_buckets):\n", "    \"\"\"Angular LSH: hash vectors by their direction.\"\"\"\n", "    d = x.shape[-1]\n", "    \n", "    # Random projection vectors\n", "    projections = torch.randn(n_hashes, d, n_buckets // 2, device=x.device)\n", "    projections = projections / projections.norm(dim=1, keepdim=True)\n", "    \n", "    # Project and sign -> bucket assignment\n", "    # x: (B, N, d) @ projections: (n_hashes, d, n_buckets//2) -> (n_hashes, B, N, n_buckets//2)\n", "    dots = torch.einsum('bnd,hdk->hbnk', x, projections)\n", "    \n", "    # Convert to bucket index: sign determines which half of buckets\n", "    signs = (dots > 0).int()\n", "    buckets = signs.sum(dim=-1)  # Simple hash combining\n", "    \n", "    return buckets  # (n_hashes, B, N)\n", "\n", "# Visualize LSH\n", "def visualize_lsh():\n", "    # Create 2D points\n", "    n_points = 100\n", "    points = torch.randn(1, n_points, 2)\n", "    \n", "    # Compute LSH buckets\n", "    buckets = angular_lsh(points, n_hashes=1, n_buckets=8)[0, 0]  # (N,)\n", "    \n", "    # Plot\n", "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "    \n", "    # Original points\n", "    ax = axes[0]\n", "    ax.scatter(points[0, :, 0], points[0, :, 1], c='gray', alpha=0.6)\n", "    ax.set_title('Original Points')\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('y')\n", "    ax.grid(True, alpha=0.3)\n", "    ax.set_aspect('equal')\n", "    \n", "    # Points colored by bucket\n", "    ax = axes[1]\n", "    scatter = ax.scatter(points[0, :, 0], points[0, :, 1], c=buckets, cmap='tab10', alpha=0.8)\n", "    ax.set_title('Points Colored by LSH Bucket')\n", "    ax.set_xlabel('x')\n", "    ax.set_ylabel('y')\n", "    ax.grid(True, alpha=0.3)\n", "    ax.set_aspect('equal')\n", "    plt.colorbar(scatter, ax=ax, label='Bucket')\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "    print('Similar vectors (close in direction) tend to be in the same bucket!')\n", "\n", "visualize_lsh()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## LSH Attention"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LSHAttention(nn.Module):\n", "    \"\"\"Locality Sensitive Hashing Attention.\"\"\"\n", "    def __init__(self, d_model, n_heads, n_buckets=64, n_hashes=4, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.n_buckets = n_buckets\n", "        self.n_hashes = n_hashes\n", "        self.scale = self.d_k ** -0.5\n", "        \n", "        # Shared Q and K (Reformer uses Q=K for LSH efficiency)\n", "        self.W_qk = nn.Linear(d_model, d_model)\n", "        self.W_v = nn.Linear(d_model, d_model)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def _lsh_hash(self, x, n_buckets):\n", "        \"\"\"Simple LSH hash function.\"\"\"\n", "        d = x.shape[-1]\n", "        # Random projections (in practice, should be fixed per layer)\n", "        projections = torch.randn(d, n_buckets // 2, device=x.device)\n", "        projections = projections / projections.norm(dim=0, keepdim=True)\n", "        \n", "        dots = x @ projections  # (B, H, N, n_buckets//2)\n", "        buckets = (dots > 0).int()\n", "        # Convert to bucket indices\n", "        powers = 2 ** torch.arange(n_buckets // 2, device=x.device)\n", "        bucket_ids = (buckets * powers).sum(dim=-1) % n_buckets\n", "        return bucket_ids\n", "    \n", "    def forward(self, x):\n", "        B, T, C = x.shape\n", "        \n", "        # Project to QK (shared) and V\n", "        qk = self.W_qk(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n", "        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n", "        \n", "        # Normalize QK for LSH\n", "        qk_normalized = F.normalize(qk, dim=-1)\n", "        \n", "        # Get bucket assignments\n", "        buckets = self._lsh_hash(qk_normalized, self.n_buckets)  # (B, H, T)\n", "        \n", "        # For simplicity, we'll do full attention here but in practice:\n", "        # Only attend within same bucket + allow attention to adjacent buckets\n", "        \n", "        # Full attention (simplified for demonstration)\n", "        attn = (qk @ qk.transpose(-2, -1)) * self.scale\n", "        \n", "        # Mask: only attend to tokens in same/similar buckets\n", "        bucket_mask = (buckets.unsqueeze(-1) == buckets.unsqueeze(-2))  # (B, H, T, T)\n", "        # Allow some cross-bucket attention by relaxing mask\n", "        attn = attn.masked_fill(~bucket_mask, -1e4)  # Soft mask\n", "        \n", "        # Causal mask\n", "        causal_mask = torch.tril(torch.ones(T, T, device=x.device)).bool()\n", "        attn = attn.masked_fill(~causal_mask, float('-inf'))\n", "        \n", "        attn = F.softmax(attn, dim=-1)\n", "        attn = self.dropout(attn)\n", "        \n", "        out = (attn @ V).transpose(1, 2).reshape(B, T, C)\n", "        return self.W_o(out), buckets\n", "\n", "# Test LSH attention\n", "lsh_attn = LSHAttention(d_model=64, n_heads=4, n_buckets=8)\n", "x = torch.randn(2, 32, 64)\n", "out, buckets = lsh_attn(x)\n", "print(f'Input: {x.shape}')\n", "print(f'Output: {out.shape}')\n", "print(f'Bucket assignments: {buckets.shape}')\n", "print(f'Unique buckets used: {buckets[0, 0].unique().shape[0]}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Reversible Layers\n", "\n", "Key memory saving: Don't store intermediate activations!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ReversibleBlock(nn.Module):\n", "    \"\"\"Reversible residual block - can recompute inputs from outputs.\"\"\"\n", "    def __init__(self, f, g):\n", "        super().__init__()\n", "        self.f = f  # Attention\n", "        self.g = g  # FFN\n", "    \n", "    def forward(self, x1, x2):\n", "        \"\"\"Forward: y1 = x1 + f(x2), y2 = x2 + g(y1)\"\"\"\n", "        y1 = x1 + self.f(x2)\n", "        y2 = x2 + self.g(y1)\n", "        return y1, y2\n", "    \n", "    def backward_pass(self, y1, y2):\n", "        \"\"\"Reverse: recover x1, x2 from y1, y2\"\"\"\n", "        x2 = y2 - self.g(y1)\n", "        x1 = y1 - self.f(x2)\n", "        return x1, x2\n", "\n", "# Demonstrate reversibility\n", "def demonstrate_reversibility():\n", "    d_model = 64\n", "    \n", "    # Simple functions for demonstration\n", "    f = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, d_model), nn.GELU())\n", "    g = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, d_model), nn.GELU())\n", "    \n", "    block = ReversibleBlock(f, g)\n", "    \n", "    # Input\n", "    x1 = torch.randn(2, 16, d_model)\n", "    x2 = torch.randn(2, 16, d_model)\n", "    \n", "    print('Original x1[:, 0, :5]:', x1[0, 0, :5].tolist())\n", "    \n", "    # Forward\n", "    with torch.no_grad():\n", "        y1, y2 = block(x1, x2)\n", "        \n", "        # Backward (reconstruct)\n", "        x1_reconstructed, x2_reconstructed = block.backward_pass(y1, y2)\n", "    \n", "    print('Reconstructed x1[:, 0, :5]:', x1_reconstructed[0, 0, :5].tolist())\n", "    \n", "    error = (x1 - x1_reconstructed).abs().max().item()\n", "    print(f'\\nReconstruction error: {error:.2e}')\n", "    print('Perfect reconstruction means no need to store activations!')\n", "\n", "demonstrate_reversibility()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Reformer Implementation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ReformerBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, n_buckets=32, dropout=0.1):\n", "        super().__init__()\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.attn = LSHAttention(d_model, n_heads, n_buckets, dropout=dropout)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.Linear(d_model, 4 * d_model),\n", "            nn.GELU(),\n", "            nn.Linear(4 * d_model, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        attn_out, _ = self.attn(self.norm1(x))\n", "        x = x + attn_out\n", "        x = x + self.ff(self.norm2(x))\n", "        return x\n", "\n", "class Reformer(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, n_buckets=32, max_len=512, dropout=0.1):\n", "        super().__init__()\n", "        self.embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = nn.Embedding(max_len, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        self.layers = nn.ModuleList([ReformerBlock(d_model, n_heads, n_buckets, dropout) for _ in range(n_layers)])\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size)\n", "    \n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n", "        \n", "        x = self.dropout(self.embed(x) + self.pos_embed(pos))\n", "        for layer in self.layers:\n", "            x = layer(x)\n", "        \n", "        return self.head(self.norm(x))\n", "\n", "model = Reformer(vocab_size=1000, d_model=64, n_heads=4, n_layers=2, n_buckets=16).to(device)\n", "print(f'Reformer Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training Reformer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Dataset\n", "text = 'the quick brown fox jumps over the lazy dog ' * 300\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n", "\n", "# Training\n", "seq_len = 64\n", "model = Reformer(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, n_buckets=16, max_len=seq_len).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "\n", "losses = []\n", "n_steps = 300\n", "\n", "print('Training Reformer with LSH attention...')\n", "for step in range(n_steps):\n", "    idx = torch.randint(0, len(data) - seq_len - 1, (16,))\n", "    x = torch.stack([data[i:i+seq_len] for i in idx]).to(device)\n", "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx]).to(device)\n", "    \n", "    optimizer.zero_grad()\n", "    logits = model(x)\n", "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    if (step + 1) % 50 == 0:\n", "        print(f'Step {step+1}: Loss = {loss.item():.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Step')\n", "plt.ylabel('Loss')\n", "plt.title('Reformer Training (LSH Attention)')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. LSH groups similar queries into buckets')\n", "print('2. Only compute attention within buckets â†’ O(N log N)')\n", "print('3. Reversible layers eliminate activation storage')\n", "print('4. Memory-efficient for very long sequences')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
