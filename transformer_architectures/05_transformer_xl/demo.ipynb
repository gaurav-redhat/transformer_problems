{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ”„ Transformer-XL\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/05_transformer_xl/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovation\n", "- **Segment-Level Recurrence**: Cache hidden states from previous segments\n", "- **Relative Positional Encoding**: Enable longer dependencies\n", "- **Extended Context**: Break fixed-length limitation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Problem: Fixed Context in Standard Transformer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Standard transformer: Each segment is processed independently\n", "# This causes context fragmentation!\n", "\n", "def visualize_context_problem():\n", "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "    \n", "    # Standard Transformer\n", "    ax = axes[0]\n", "    segments = ['Seg 1', 'Seg 2', 'Seg 3', 'Seg 4']\n", "    for i, seg in enumerate(segments):\n", "        rect = plt.Rectangle((i*1.1, 0), 1, 0.5, color=plt.cm.Blues(0.3+i*0.15))\n", "        ax.add_patch(rect)\n", "        ax.text(i*1.1+0.5, 0.25, seg, ha='center', va='center', fontsize=10)\n", "        # No arrows between segments\n", "    ax.set_xlim(-0.2, 5)\n", "    ax.set_ylim(-0.5, 1)\n", "    ax.set_title('Standard Transformer: No Context Across Segments', fontsize=12)\n", "    ax.axis('off')\n", "    ax.text(2.2, -0.3, 'X No memory between segments', fontsize=11, color='red', ha='center')\n", "    \n", "    # Transformer-XL\n", "    ax = axes[1]\n", "    for i, seg in enumerate(segments):\n", "        rect = plt.Rectangle((i*1.1, 0), 1, 0.5, color=plt.cm.Greens(0.3+i*0.15))\n", "        ax.add_patch(rect)\n", "        ax.text(i*1.1+0.5, 0.25, seg, ha='center', va='center', fontsize=10)\n", "        if i > 0:\n", "            ax.annotate('', xy=(i*1.1+0.1, 0.5), xytext=((i-1)*1.1+0.9, 0.5),\n", "                       arrowprops=dict(arrowstyle='->', color='green', lw=2))\n", "    ax.set_xlim(-0.2, 5)\n", "    ax.set_ylim(-0.5, 1)\n", "    ax.set_title('Transformer-XL: Recurrent Memory', fontsize=12)\n", "    ax.axis('off')\n", "    ax.text(2.2, -0.3, 'âœ“ Hidden states cached and reused', fontsize=11, color='green', ha='center')\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "visualize_context_problem()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Relative Positional Encoding"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RelativePositionalEncoding(nn.Module):\n", "    \"\"\"Relative positional encoding for Transformer-XL.\"\"\"\n", "    def __init__(self, d_model, max_len=1024):\n", "        super().__init__()\n", "        self.d_model = d_model\n", "        \n", "        # Create sinusoidal positional encodings for relative positions\n", "        pe = torch.zeros(max_len, d_model)\n", "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n", "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n", "        pe[:, 0::2] = torch.sin(position * div_term)\n", "        pe[:, 1::2] = torch.cos(position * div_term)\n", "        self.register_buffer('pe', pe)\n", "    \n", "    def forward(self, seq_len, mem_len):\n", "        # Returns encodings for relative positions\n", "        total_len = seq_len + mem_len\n", "        return self.pe[:total_len]\n", "\n", "# Visualize relative vs absolute positioning\n", "def compare_positions():\n", "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n", "    \n", "    # Absolute positions (problematic for long sequences)\n", "    ax = axes[0]\n", "    positions = list(range(8))\n", "    ax.barh(positions, [1]*8, color='coral')\n", "    for i, p in enumerate(positions):\n", "        ax.text(0.5, i, f'pos={p}', va='center', ha='center', fontsize=10)\n", "    ax.set_title('Absolute Position: Fixed for each location', fontsize=11)\n", "    ax.set_xlabel('Token Index')\n", "    ax.invert_yaxis()\n", "    \n", "    # Relative positions\n", "    ax = axes[1]\n", "    query_pos = 4\n", "    rel_positions = [query_pos - i for i in range(8)]\n", "    colors = plt.cm.RdYlGn([0.5 + 0.05*p for p in rel_positions])\n", "    ax.barh(range(8), [1]*8, color=colors)\n", "    for i, p in enumerate(rel_positions):\n", "        ax.text(0.5, i, f'rel={p}', va='center', ha='center', fontsize=10)\n", "    ax.axhline(y=query_pos, color='blue', linestyle='--', linewidth=2, label='Query')\n", "    ax.set_title(f'Relative Position: Distance from query (pos={query_pos})', fontsize=11)\n", "    ax.set_xlabel('Token Index')\n", "    ax.legend()\n", "    ax.invert_yaxis()\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "compare_positions()\n", "print('Relative positions enable: Same pattern at any location!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Transformer-XL Architecture"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class TransformerXLAttention(nn.Module):\n", "    \"\"\"Multi-head attention with segment-level recurrence.\"\"\"\n", "    def __init__(self, d_model, n_heads, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.scale = self.d_k ** -0.5\n", "        \n", "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n", "        self.W_kv = nn.Linear(d_model, 2 * d_model, bias=False)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        \n", "        # Relative position biases (from Transformer-XL paper)\n", "        self.u = nn.Parameter(torch.randn(n_heads, self.d_k) * 0.02)\n", "        self.v = nn.Parameter(torch.randn(n_heads, self.d_k) * 0.02)\n", "        \n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, h, memory=None, pos_embed=None, mask=None):\n", "        B, T, C = h.shape\n", "        \n", "        # Concatenate memory (cached hidden states from previous segment)\n", "        if memory is not None:\n", "            cat = torch.cat([memory, h], dim=1)  # (B, M+T, C)\n", "        else:\n", "            cat = h\n", "        \n", "        M = cat.size(1) - T  # Memory length\n", "        \n", "        # Q from current segment, K/V from memory + current\n", "        Q = self.W_q(h).view(B, T, self.n_heads, self.d_k)  # (B, T, H, D)\n", "        kv = self.W_kv(cat).view(B, -1, 2, self.n_heads, self.d_k)  # (B, M+T, 2, H, D)\n", "        K, V = kv[:, :, 0], kv[:, :, 1]  # (B, M+T, H, D)\n", "        \n", "        # Content-based attention\n", "        Q_u = Q + self.u  # Add content bias\n", "        content_score = torch.einsum('bthd,bshd->btsh', Q_u, K)  # (B, T, M+T, H)\n", "        \n", "        # Position-based attention (simplified)\n", "        if pos_embed is not None:\n", "            Q_v = Q + self.v  # Add position bias\n", "            pos_score = torch.einsum('bthd,sd->btsh', Q_v, pos_embed[:M+T])  # Simplified\n", "            attn = (content_score + pos_score.unsqueeze(-1)) * self.scale\n", "        else:\n", "            attn = content_score * self.scale\n", "        \n", "        # Apply causal mask\n", "        if mask is not None:\n", "            attn = attn.masked_fill(mask[:T, :M+T].unsqueeze(0).unsqueeze(-1) == 0, float('-inf'))\n", "        \n", "        attn = F.softmax(attn, dim=2)\n", "        attn = self.dropout(attn)\n", "        \n", "        out = torch.einsum('btsh,bshd->bthd', attn, V).reshape(B, T, C)\n", "        return self.W_o(out)\n", "\n", "class TransformerXLBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):\n", "        super().__init__()\n", "        d_ff = d_ff or 4 * d_model\n", "        self.attn = TransformerXLAttention(d_model, n_heads, dropout)\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.Linear(d_model, d_ff),\n", "            nn.GELU(),\n", "            nn.Dropout(dropout),\n", "            nn.Linear(d_ff, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x, memory=None, pos_embed=None, mask=None):\n", "        x = x + self.attn(self.norm1(x), memory, pos_embed, mask)\n", "        x = x + self.ff(self.norm2(x))\n", "        return x\n", "\n", "class TransformerXL(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, mem_len=64, dropout=0.1):\n", "        super().__init__()\n", "        self.d_model = d_model\n", "        self.mem_len = mem_len\n", "        self.n_layers = n_layers\n", "        \n", "        self.embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = RelativePositionalEncoding(d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        self.layers = nn.ModuleList([TransformerXLBlock(d_model, n_heads, dropout=dropout) for _ in range(n_layers)])\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size)\n", "    \n", "    def forward(self, x, memories=None):\n", "        B, T = x.shape\n", "        \n", "        h = self.dropout(self.embed(x))\n", "        \n", "        if memories is None:\n", "            memories = [None] * self.n_layers\n", "        \n", "        new_memories = []\n", "        mem_len = memories[0].size(1) if memories[0] is not None else 0\n", "        pos = self.pos_embed.pe[:T + mem_len]\n", "        \n", "        # Create causal mask\n", "        mask = torch.tril(torch.ones(T + mem_len, T + mem_len, device=x.device))\n", "        \n", "        for i, layer in enumerate(self.layers):\n", "            # Cache current hidden state for next segment\n", "            new_memories.append(h.detach()[:, -self.mem_len:] if self.mem_len > 0 else None)\n", "            h = layer(h, memories[i], pos, mask)\n", "        \n", "        h = self.norm(h)\n", "        return self.head(h), new_memories\n", "\n", "model = TransformerXL(vocab_size=1000, d_model=64, n_heads=4, n_layers=3, mem_len=32).to(device)\n", "print(f'Transformer-XL Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training with Segment-Level Recurrence"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a simple sequence dataset\n", "text = 'the quick brown fox jumps over the lazy dog ' * 500\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "idx_to_char = {i: c for i, c in enumerate(chars)}\n", "\n", "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n", "print(f'Vocab: {len(chars)}, Data: {len(data)}')\n", "\n", "# Training with memory\n", "model = TransformerXL(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, mem_len=32).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "\n", "seg_len = 32\n", "n_steps = 300\n", "losses = []\n", "\n", "print('\\nTraining Transformer-XL with recurrent memory...')\n", "for step in range(n_steps):\n", "    # Random starting point\n", "    start = torch.randint(0, len(data) - seg_len * 3, (1,)).item()\n", "    \n", "    total_loss = 0\n", "    memories = None\n", "    \n", "    # Process multiple segments with memory\n", "    for seg_idx in range(3):\n", "        seg_start = start + seg_idx * seg_len\n", "        x = data[seg_start:seg_start + seg_len].unsqueeze(0).to(device)\n", "        y = data[seg_start + 1:seg_start + seg_len + 1].unsqueeze(0).to(device)\n", "        \n", "        logits, memories = model(x, memories)\n", "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "        total_loss += loss\n", "        \n", "        # Detach memories to prevent backprop through time (BPTT)\n", "        memories = [m.detach() if m is not None else None for m in memories]\n", "    \n", "    optimizer.zero_grad()\n", "    total_loss.backward()\n", "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n", "    optimizer.step()\n", "    \n", "    losses.append(total_loss.item() / 3)\n", "    \n", "    if (step + 1) % 50 == 0:\n", "        print(f'Step {step+1}: Loss = {losses[-1]:.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Step')\n", "plt.ylabel('Loss')\n", "plt.title('Transformer-XL Training with Recurrent Memory')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare: Standard vs XL context\n", "def visualize_effective_context():\n", "    fig, ax = plt.subplots(figsize=(12, 4))\n", "    \n", "    # Standard Transformer\n", "    seg_len = 32\n", "    n_segs = 4\n", "    \n", "    # Colors for each segment\n", "    colors = plt.cm.tab10(range(n_segs))\n", "    \n", "    x_pos = 0\n", "    for i in range(n_segs):\n", "        rect = plt.Rectangle((x_pos, 0.5), seg_len, 0.4, color=colors[i], alpha=0.6)\n", "        ax.add_patch(rect)\n", "        ax.text(x_pos + seg_len/2, 0.7, f'Seg {i+1}', ha='center', va='center')\n", "        \n", "        # Standard transformer context (only current segment)\n", "        ax.arrow(x_pos + seg_len/2, 0.3, 0, -0.1, head_width=2, head_length=0.02, fc='gray', ec='gray')\n", "        ax.text(x_pos + seg_len/2, 0.1, f'Context: {seg_len}', ha='center', fontsize=8)\n", "        \n", "        x_pos += seg_len + 5\n", "    \n", "    ax.text(65, 0.1, 'Standard Transformer', fontsize=10, color='gray')\n", "    \n", "    # Transformer-XL\n", "    x_pos = 0\n", "    for i in range(n_segs):\n", "        rect = plt.Rectangle((x_pos, -0.5), seg_len, 0.4, color=colors[i], alpha=0.6)\n", "        ax.add_patch(rect)\n", "        ax.text(x_pos + seg_len/2, -0.3, f'Seg {i+1}', ha='center', va='center')\n", "        \n", "        # XL context (current + memory from previous)\n", "        mem_ctx = min(i, 2) * seg_len + seg_len  # Growing context\n", "        ax.arrow(x_pos + seg_len/2, -0.7, 0, -0.1, head_width=2, head_length=0.02, fc='green', ec='green')\n", "        ax.text(x_pos + seg_len/2, -0.9, f'Context: {mem_ctx}', ha='center', fontsize=8, color='green')\n", "        \n", "        x_pos += seg_len + 5\n", "    \n", "    ax.text(65, -0.9, 'Transformer-XL', fontsize=10, color='green')\n", "    \n", "    ax.set_xlim(-5, 150)\n", "    ax.set_ylim(-1.1, 1.1)\n", "    ax.set_title('Effective Context Length Comparison')\n", "    ax.axis('off')\n", "    plt.show()\n", "\n", "visualize_effective_context()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. Transformer-XL caches hidden states from previous segments')\n", "print('2. Relative positional encoding enables unbounded context')\n", "print('3. Effective context grows with each segment')\n", "print('4. Essential for long document understanding')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
