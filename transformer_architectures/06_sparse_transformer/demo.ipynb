{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# âš¡ Sparse Transformer\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/06_sparse_transformer/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovation\n", "- **Sparse Attention Patterns**: Attend to subset of tokens\n", "- **O(NâˆšN) Complexity**: vs O(NÂ²) for standard attention\n", "- **Scalable**: Handle much longer sequences"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Problem: Quadratic Attention Complexity"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Memory and FLOPs comparison\n", "def compute_complexity(seq_lengths):\n", "    standard = [n**2 for n in seq_lengths]\n", "    sparse = [n * math.sqrt(n) for n in seq_lengths]\n", "    return standard, sparse\n", "\n", "seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n", "standard, sparse = compute_complexity(seq_lengths)\n", "\n", "plt.figure(figsize=(10, 5))\n", "plt.plot(seq_lengths, standard, 'ro-', label='Standard O(NÂ²)', linewidth=2)\n", "plt.plot(seq_lengths, sparse, 'go-', label='Sparse O(NâˆšN)', linewidth=2)\n", "plt.xlabel('Sequence Length')\n", "plt.ylabel('Operations (relative)')\n", "plt.title('Attention Complexity Comparison')\n", "plt.legend()\n", "plt.grid(True, alpha=0.3)\n", "plt.yscale('log')\n", "plt.show()\n", "\n", "print(f'\\nAt seq_len=8192:')\n", "print(f'  Standard: {standard[-1]:,.0f} operations')\n", "print(f'  Sparse: {sparse[-1]:,.0f} operations')\n", "print(f'  Speedup: {standard[-1]/sparse[-1]:.1f}x')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Sparse Attention Patterns"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_attention_patterns(seq_len):\n", "    \"\"\"Create different sparse attention patterns.\"\"\"\n", "    patterns = {}\n", "    \n", "    # 1. Full attention (standard)\n", "    full = torch.ones(seq_len, seq_len)\n", "    full = torch.tril(full)  # Causal\n", "    patterns['Full (O(NÂ²))'] = full\n", "    \n", "    # 2. Local attention (sliding window)\n", "    window_size = seq_len // 4\n", "    local = torch.zeros(seq_len, seq_len)\n", "    for i in range(seq_len):\n", "        start = max(0, i - window_size)\n", "        local[i, start:i+1] = 1\n", "    patterns['Local Window'] = local\n", "    \n", "    # 3. Strided attention (every k-th token)\n", "    stride = int(math.sqrt(seq_len))\n", "    strided = torch.zeros(seq_len, seq_len)\n", "    for i in range(seq_len):\n", "        for j in range(0, i+1, stride):\n", "            strided[i, j] = 1\n", "    patterns['Strided'] = strided\n", "    \n", "    # 4. Combined (Sparse Transformer): Local + Strided\n", "    combined = (local + strided).clamp(0, 1)\n", "    patterns['Sparse (Local+Strided)'] = combined\n", "    \n", "    # 5. Fixed positions (attend to first few tokens)\n", "    fixed = local.clone()\n", "    fixed[:, :4] = 1  # Always attend to first 4 tokens\n", "    fixed = torch.tril(fixed)\n", "    patterns['Fixed + Local'] = fixed\n", "    \n", "    return patterns\n", "\n", "# Visualize patterns\n", "seq_len = 32\n", "patterns = create_attention_patterns(seq_len)\n", "\n", "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n", "for ax, (name, pattern) in zip(axes, patterns.items()):\n", "    ax.imshow(pattern, cmap='Blues')\n", "    ax.set_title(f'{name}\\nDensity: {pattern.sum()/(seq_len**2)*100:.1f}%')\n", "    ax.set_xlabel('Key')\n", "    ax.set_ylabel('Query')\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Sparse Transformer Implementation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SparseAttention(nn.Module):\n", "    \"\"\"Sparse attention with configurable pattern.\"\"\"\n", "    def __init__(self, d_model, n_heads, seq_len, window_size=None, stride=None, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.scale = self.d_k ** -0.5\n", "        self.seq_len = seq_len\n", "        \n", "        self.W_qkv = nn.Linear(d_model, 3 * d_model)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        # Create sparse mask\n", "        window_size = window_size or int(math.sqrt(seq_len))\n", "        stride = stride or int(math.sqrt(seq_len))\n", "        \n", "        mask = self._create_sparse_mask(seq_len, window_size, stride)\n", "        self.register_buffer('sparse_mask', mask)\n", "    \n", "    def _create_sparse_mask(self, seq_len, window_size, stride):\n", "        \"\"\"Combined local + strided pattern.\"\"\"\n", "        mask = torch.zeros(seq_len, seq_len)\n", "        \n", "        for i in range(seq_len):\n", "            # Local: attend to nearby tokens\n", "            start = max(0, i - window_size)\n", "            mask[i, start:i+1] = 1\n", "            \n", "            # Strided: attend to every stride-th token\n", "            for j in range(0, i+1, stride):\n", "                mask[i, j] = 1\n", "        \n", "        return mask\n", "    \n", "    def forward(self, x):\n", "        B, T, C = x.shape\n", "        \n", "        # QKV projection\n", "        qkv = self.W_qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n", "        Q, K, V = qkv[0], qkv[1], qkv[2]\n", "        \n", "        # Compute attention scores\n", "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n", "        \n", "        # Apply sparse mask\n", "        mask = self.sparse_mask[:T, :T]\n", "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0) == 0, float('-inf'))\n", "        \n", "        attn = F.softmax(attn, dim=-1)\n", "        attn = self.dropout(attn)\n", "        \n", "        out = (attn @ V).transpose(1, 2).reshape(B, T, C)\n", "        return self.W_o(out), attn\n", "\n", "class SparseTransformerBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, seq_len, dropout=0.1):\n", "        super().__init__()\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.attn = SparseAttention(d_model, n_heads, seq_len, dropout=dropout)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.Linear(d_model, 4 * d_model),\n", "            nn.GELU(),\n", "            nn.Linear(4 * d_model, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        attn_out, attn_weights = self.attn(self.norm1(x))\n", "        x = x + attn_out\n", "        x = x + self.ff(self.norm2(x))\n", "        return x, attn_weights\n", "\n", "class SparseTransformer(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, seq_len=256, dropout=0.1):\n", "        super().__init__()\n", "        self.embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = nn.Embedding(seq_len, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        self.layers = nn.ModuleList([SparseTransformerBlock(d_model, n_heads, seq_len, dropout) for _ in range(n_layers)])\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size)\n", "    \n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n", "        \n", "        x = self.dropout(self.embed(x) + self.pos_embed(pos))\n", "        \n", "        attn_weights = []\n", "        for layer in self.layers:\n", "            x, attn = layer(x)\n", "            attn_weights.append(attn)\n", "        \n", "        x = self.norm(x)\n", "        return self.head(x), attn_weights\n", "\n", "model = SparseTransformer(vocab_size=1000, d_model=64, n_heads=4, n_layers=2, seq_len=128).to(device)\n", "print(f'Sparse Transformer Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training on Longer Sequences"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create dataset\n", "text = 'the quick brown fox jumps over the lazy dog and the cat sleeps on the mat ' * 200\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "idx_to_char = {i: c for i, c in enumerate(chars)}\n", "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n", "\n", "# Training\n", "seq_len = 128  # Longer sequence!\n", "model = SparseTransformer(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, seq_len=seq_len).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "\n", "losses = []\n", "n_steps = 300\n", "batch_size = 16\n", "\n", "print(f'Training Sparse Transformer (seq_len={seq_len})...')\n", "for step in range(n_steps):\n", "    idx = torch.randint(0, len(data) - seq_len - 1, (batch_size,))\n", "    x = torch.stack([data[i:i+seq_len] for i in idx]).to(device)\n", "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx]).to(device)\n", "    \n", "    optimizer.zero_grad()\n", "    logits, _ = model(x)\n", "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    \n", "    if (step + 1) % 50 == 0:\n", "        print(f'Step {step+1}: Loss = {loss.item():.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Step')\n", "plt.ylabel('Loss')\n", "plt.title('Sparse Transformer Training')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize learned attention pattern\n", "model.eval()\n", "test_input = torch.randint(0, vocab_size, (1, 64)).to(device)\n", "\n", "with torch.no_grad():\n", "    _, attn_weights = model(test_input)\n", "\n", "# Plot attention from last layer, first head\n", "attn = attn_weights[-1][0, 0].cpu()\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Sparse mask\n", "axes[0].imshow(model.layers[-1].attn.sparse_mask[:64, :64].cpu(), cmap='Greys', alpha=0.3)\n", "axes[0].set_title('Sparse Attention Mask')\n", "axes[0].set_xlabel('Key')\n", "axes[0].set_ylabel('Query')\n", "\n", "# Actual attention weights\n", "axes[1].imshow(attn, cmap='Blues')\n", "axes[1].set_title('Learned Attention Weights')\n", "axes[1].set_xlabel('Key')\n", "axes[1].set_ylabel('Query')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. Sparse patterns reduce O(NÂ²) to O(NâˆšN)')\n", "print('2. Local + strided captures both nearby and distant relationships')\n", "print('3. Enables processing of much longer sequences')\n", "print('4. Used in GPT-3 and other large models for efficiency')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
