{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ“š BERT: Bidirectional Encoder Representations\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/transformer/transformer_architectures/02_bert/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovations\n", "- **Bidirectional**: See left AND right context\n", "- **MLM**: Masked Language Modeling pretraining\n", "- **NSP**: Next Sentence Prediction"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## BERT Architecture"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BertEmbeddings(nn.Module):\n", "    \"\"\"BERT has 3 embeddings: Token + Position + Segment\"\"\"\n", "    def __init__(self, vocab_size, d_model, max_len=512, n_segments=2, dropout=0.1):\n", "        super().__init__()\n", "        self.token_embed = nn.Embedding(vocab_size, d_model)\n", "        self.position_embed = nn.Embedding(max_len, d_model)\n", "        self.segment_embed = nn.Embedding(n_segments, d_model)\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, tokens, segments=None):\n", "        seq_len = tokens.size(1)\n", "        positions = torch.arange(seq_len, device=tokens.device).unsqueeze(0)\n", "        \n", "        if segments is None:\n", "            segments = torch.zeros_like(tokens)\n", "        \n", "        x = self.token_embed(tokens) + self.position_embed(positions) + self.segment_embed(segments)\n", "        return self.dropout(self.norm(x))\n", "\n", "class BertAttention(nn.Module):\n", "    def __init__(self, d_model, n_heads, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.W_qkv = nn.Linear(d_model, 3 * d_model)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, x, mask=None):\n", "        B, L, D = x.shape\n", "        qkv = self.W_qkv(x).reshape(B, L, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n", "        Q, K, V = qkv[0], qkv[1], qkv[2]\n", "        \n", "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n", "        if mask is not None:\n", "            scores = scores.masked_fill(mask == 0, float('-inf'))\n", "        \n", "        attn = self.dropout(F.softmax(scores, dim=-1))\n", "        out = torch.matmul(attn, V).transpose(1, 2).reshape(B, L, D)\n", "        return self.W_o(out), attn\n", "\n", "class BertBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n", "        super().__init__()\n", "        self.attn = BertAttention(d_model, n_heads, dropout)\n", "        self.ffn = nn.Sequential(\n", "            nn.Linear(d_model, d_ff),\n", "            nn.GELU(),\n", "            nn.Linear(d_ff, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, x, mask=None):\n", "        attn_out, _ = self.attn(x, mask)\n", "        x = self.norm1(x + self.dropout(attn_out))\n", "        x = self.norm2(x + self.ffn(x))\n", "        return x\n", "\n", "class BERT(nn.Module):\n", "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4, d_ff=512, dropout=0.1):\n", "        super().__init__()\n", "        self.embed = BertEmbeddings(vocab_size, d_model, dropout=dropout)\n", "        self.blocks = nn.ModuleList([BertBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n", "        self.mlm_head = nn.Linear(d_model, vocab_size)\n", "        self.cls_head = nn.Linear(d_model, 2)  # For NSP\n", "    \n", "    def forward(self, tokens, segments=None, mask=None):\n", "        x = self.embed(tokens, segments)\n", "        for block in self.blocks:\n", "            x = block(x, mask)\n", "        return x\n", "    \n", "    def mlm_forward(self, tokens, segments=None, mask=None):\n", "        x = self.forward(tokens, segments, mask)\n", "        return self.mlm_head(x)\n", "    \n", "    def cls_forward(self, tokens, segments=None, mask=None):\n", "        x = self.forward(tokens, segments, mask)\n", "        return self.cls_head(x[:, 0])  # [CLS] token\n", "\n", "model = BERT(vocab_size=5000, d_model=128, n_heads=4, n_layers=3)\n", "print(f'BERT Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Masked Language Modeling (MLM)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_mlm_data(tokens, vocab_size, mask_prob=0.15, mask_token=4):\n", "    \"\"\"\n", "    Create MLM training data:\n", "    - 80%: Replace with [MASK]\n", "    - 10%: Replace with random token\n", "    - 10%: Keep original\n", "    \"\"\"\n", "    labels = tokens.clone()\n", "    masked_tokens = tokens.clone()\n", "    \n", "    # Create mask (don't mask special tokens 0-4)\n", "    mask_candidates = (tokens > 4).float()\n", "    mask_probs = torch.rand_like(tokens.float()) * mask_candidates\n", "    mask_positions = mask_probs < mask_prob\n", "    \n", "    # Set labels for non-masked positions to -100 (ignore)\n", "    labels[~mask_positions] = -100\n", "    \n", "    # Apply masking strategy\n", "    rand = torch.rand_like(tokens.float())\n", "    \n", "    # 80% -> [MASK]\n", "    mask_token_positions = mask_positions & (rand < 0.8)\n", "    masked_tokens[mask_token_positions] = mask_token\n", "    \n", "    # 10% -> random token\n", "    random_positions = mask_positions & (rand >= 0.8) & (rand < 0.9)\n", "    masked_tokens[random_positions] = torch.randint(5, vocab_size, (random_positions.sum(),))\n", "    \n", "    # 10% -> keep original (do nothing)\n", "    \n", "    return masked_tokens, labels\n", "\n", "# Test MLM\n", "tokens = torch.randint(5, 100, (2, 10))\n", "masked, labels = create_mlm_data(tokens, vocab_size=100)\n", "print('Original:', tokens[0].tolist())\n", "print('Masked:  ', masked[0].tolist())\n", "print('Labels:  ', labels[0].tolist())"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training on Tiny Dataset"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create tiny text dataset\n", "class TinyTextDataset:\n", "    def __init__(self, vocab_size=1000, size=2000, max_len=32):\n", "        self.vocab_size = vocab_size\n", "        self.max_len = max_len\n", "        # Generate random sequences (simulating tokenized text)\n", "        self.data = [torch.randint(5, vocab_size, (np.random.randint(10, max_len),)) for _ in range(size)]\n", "    \n", "    def get_batch(self, batch_size):\n", "        indices = np.random.choice(len(self.data), batch_size)\n", "        batch = [self.data[i] for i in indices]\n", "        \n", "        # Pad\n", "        max_len = max(len(x) for x in batch)\n", "        padded = torch.zeros(batch_size, max_len, dtype=torch.long)\n", "        attention_mask = torch.zeros(batch_size, max_len)\n", "        \n", "        for i, seq in enumerate(batch):\n", "            # Add [CLS] at start\n", "            padded[i, 0] = 1  # [CLS]\n", "            padded[i, 1:len(seq)+1] = seq[:max_len-1]\n", "            attention_mask[i, :len(seq)+1] = 1\n", "        \n", "        return padded, attention_mask\n", "\n", "dataset = TinyTextDataset(vocab_size=1000, size=3000)\n", "print(f'Dataset size: {len(dataset.data)} sequences')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training\n", "model = BERT(vocab_size=1000, d_model=128, n_heads=4, n_layers=3, d_ff=256).to(device)\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n", "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n", "\n", "losses = []\n", "n_epochs = 100\n", "batch_size = 32\n", "\n", "print('Training BERT with MLM...')\n", "for epoch in range(n_epochs):\n", "    model.train()\n", "    \n", "    tokens, attn_mask = dataset.get_batch(batch_size)\n", "    tokens = tokens.to(device)\n", "    \n", "    # Create MLM data\n", "    masked_tokens, mlm_labels = create_mlm_data(tokens, vocab_size=1000)\n", "    masked_tokens = masked_tokens.to(device)\n", "    mlm_labels = mlm_labels.to(device)\n", "    \n", "    # Forward\n", "    optimizer.zero_grad()\n", "    logits = model.mlm_forward(masked_tokens)\n", "    \n", "    loss = criterion(logits.view(-1, 1000), mlm_labels.view(-1))\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    \n", "    if (epoch + 1) % 20 == 0:\n", "        print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Epoch')\n", "plt.ylabel('MLM Loss')\n", "plt.title('BERT MLM Training')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Test: Fill in the mask\n", "model.eval()\n", "\n", "test_tokens = torch.randint(5, 100, (1, 15)).to(device)\n", "test_tokens[0, 5] = 4  # [MASK] at position 5\n", "\n", "with torch.no_grad():\n", "    logits = model.mlm_forward(test_tokens)\n", "    predicted = logits[0, 5].argmax().item()\n", "\n", "print(f'Input (with MASK at pos 5): {test_tokens[0].tolist()}')\n", "print(f'Predicted token for MASK: {predicted}')\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. BERT is encoder-only (no autoregressive generation)')\n", "print('2. Bidirectional: Can see full context in both directions')\n", "print('3. MLM: Learn by predicting masked tokens')\n", "print('4. Great for classification, NER, QA tasks')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
