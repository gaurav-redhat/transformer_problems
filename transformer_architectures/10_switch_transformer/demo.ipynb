{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ”€ Switch Transformer: Mixture of Experts\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/10_switch_transformer/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovation\n", "- **Mixture of Experts (MoE)**: Multiple FFN \"experts\"\n", "- **Sparse Activation**: Each token routed to 1 expert\n", "- **Massive Scale**: Trillion parameters with constant FLOPs"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## The MoE Concept\n", "\n", "Instead of one large FFN, use many small \"expert\" FFNs and route tokens!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_moe_concept():\n", "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "    \n", "    # Standard FFN\n", "    ax = axes[0]\n", "    ax.add_patch(plt.Rectangle((0.2, 0.3), 0.6, 0.4, color='steelblue', alpha=0.7))\n", "    ax.text(0.5, 0.5, 'Single Large FFN\\n(d_ff = 4096)', ha='center', va='center', fontsize=12, color='white')\n", "    \n", "    # Input tokens\n", "    for i, y in enumerate([0.15, 0.1, 0.05]):\n", "        ax.annotate('', xy=(0.2, 0.3 + i*0.1), xytext=(0.05, y + 0.1),\n", "                   arrowprops=dict(arrowstyle='->', color='gray'))\n", "        ax.text(0.02, y + 0.1, f'Token {i+1}', fontsize=9)\n", "    \n", "    ax.set_xlim(0, 1)\n", "    ax.set_ylim(0, 1)\n", "    ax.set_title('Standard Transformer\\nAll tokens â†’ One FFN', fontsize=11)\n", "    ax.axis('off')\n", "    \n", "    # MoE\n", "    ax = axes[1]\n", "    colors = plt.cm.Set3(range(4))\n", "    expert_positions = [(0.15, 0.5), (0.4, 0.5), (0.6, 0.5), (0.85, 0.5)]\n", "    \n", "    for i, (x, y) in enumerate(expert_positions):\n", "        ax.add_patch(plt.Rectangle((x-0.08, y-0.15), 0.16, 0.3, color=colors[i], alpha=0.8))\n", "        ax.text(x, y, f'Expert\\n{i+1}', ha='center', va='center', fontsize=9)\n", "    \n", "    # Router\n", "    ax.add_patch(plt.Circle((0.5, 0.2), 0.08, color='gold', alpha=0.8))\n", "    ax.text(0.5, 0.2, 'Router', ha='center', va='center', fontsize=9)\n", "    \n", "    # Tokens being routed\n", "    token_routes = [(0, 0), (1, 2), (2, 1)]  # token -> expert\n", "    for tok_idx, exp_idx in token_routes:\n", "        ax.annotate('', xy=(expert_positions[exp_idx][0], 0.35),\n", "                   xytext=(0.5, 0.28),\n", "                   arrowprops=dict(arrowstyle='->', color=colors[exp_idx], lw=2))\n", "    \n", "    ax.set_xlim(0, 1)\n", "    ax.set_ylim(0, 1)\n", "    ax.set_title('Switch Transformer\\nTokens routed to different experts', fontsize=11)\n", "    ax.axis('off')\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "visualize_moe_concept()\n", "print('Key: More parameters, same compute per token!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Switch Routing"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SwitchRouter(nn.Module):\n", "    \"\"\"Route tokens to experts using learned routing.\"\"\"\n", "    def __init__(self, d_model, n_experts, capacity_factor=1.25):\n", "        super().__init__()\n", "        self.n_experts = n_experts\n", "        self.capacity_factor = capacity_factor\n", "        self.router = nn.Linear(d_model, n_experts)\n", "    \n", "    def forward(self, x):\n", "        \"\"\"Route tokens to experts.\n", "        \n", "        Returns:\n", "            - expert_indices: Which expert each token goes to\n", "            - router_probs: Probability for top expert (for loss)\n", "            - expert_mask: Binary mask for each expert\n", "        \"\"\"\n", "        B, T, C = x.shape\n", "        \n", "        # Compute routing scores\n", "        router_logits = self.router(x)  # (B, T, n_experts)\n", "        router_probs = F.softmax(router_logits, dim=-1)\n", "        \n", "        # Select top expert for each token (Switch uses top-1)\n", "        expert_indices = router_probs.argmax(dim=-1)  # (B, T)\n", "        top_probs = router_probs.max(dim=-1).values  # (B, T)\n", "        \n", "        # Create mask for each expert\n", "        expert_mask = F.one_hot(expert_indices, self.n_experts).float()  # (B, T, n_experts)\n", "        \n", "        return expert_indices, top_probs, expert_mask, router_probs\n", "\n", "# Demonstrate routing\n", "router = SwitchRouter(d_model=64, n_experts=4)\n", "x = torch.randn(2, 16, 64)  # Batch=2, SeqLen=16\n", "\n", "expert_indices, top_probs, expert_mask, router_probs = router(x)\n", "\n", "print(f'Input shape: {x.shape}')\n", "print(f'Expert indices shape: {expert_indices.shape}')\n", "print(f'\\nToken routing (batch 0):')\n", "print(f'  Expert assignments: {expert_indices[0].tolist()}')\n", "print(f'  Routing probs: {top_probs[0, :5].tolist()}')  # First 5 tokens\n", "\n", "# Visualize expert distribution\n", "expert_counts = expert_mask[0].sum(dim=0).tolist()\n", "plt.figure(figsize=(8, 4))\n", "plt.bar(range(4), expert_counts, color=plt.cm.Set3(range(4)))\n", "plt.xlabel('Expert ID')\n", "plt.ylabel('Number of Tokens')\n", "plt.title('Token Distribution Across Experts')\n", "plt.xticks(range(4), [f'Expert {i}' for i in range(4)])\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Load Balancing Loss\n", "\n", "Prevent all tokens going to one expert!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_balancing_loss(router_probs, expert_mask):\n", "    \"\"\"Auxiliary loss to encourage balanced routing.\n", "    \n", "    From Switch Transformer paper:\n", "    L_aux = alpha * n_experts * sum_i(f_i * P_i)\n", "    where f_i = fraction of tokens to expert i\n", "          P_i = average routing probability for expert i\n", "    \"\"\"\n", "    # Fraction of tokens routed to each expert\n", "    tokens_per_expert = expert_mask.sum(dim=(0, 1))  # (n_experts,)\n", "    total_tokens = expert_mask.sum()\n", "    f = tokens_per_expert / (total_tokens + 1e-10)\n", "    \n", "    # Average routing probability for each expert\n", "    P = router_probs.mean(dim=(0, 1))  # (n_experts,)\n", "    \n", "    # Load balancing loss\n", "    n_experts = router_probs.shape[-1]\n", "    loss = n_experts * (f * P).sum()\n", "    \n", "    return loss\n", "\n", "# Test load balancing\n", "lb_loss = load_balancing_loss(router_probs, expert_mask)\n", "print(f'Load balancing loss: {lb_loss.item():.4f}')\n", "print('(Ideal is 1.0 when perfectly balanced)')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Switch Transformer Implementation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Expert(nn.Module):\n", "    \"\"\"Single FFN expert.\"\"\"\n", "    def __init__(self, d_model, d_ff, dropout=0.1):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Linear(d_model, d_ff),\n", "            nn.GELU(),\n", "            nn.Dropout(dropout),\n", "            nn.Linear(d_ff, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        return self.net(x)\n", "\n", "class SwitchFFN(nn.Module):\n", "    \"\"\"Switch Transformer FFN with multiple experts.\"\"\"\n", "    def __init__(self, d_model, d_ff, n_experts=4, dropout=0.1):\n", "        super().__init__()\n", "        self.n_experts = n_experts\n", "        self.router = SwitchRouter(d_model, n_experts)\n", "        self.experts = nn.ModuleList([Expert(d_model, d_ff, dropout) for _ in range(n_experts)])\n", "    \n", "    def forward(self, x):\n", "        B, T, C = x.shape\n", "        \n", "        # Get routing\n", "        expert_indices, top_probs, expert_mask, router_probs = self.router(x)\n", "        \n", "        # Process tokens by their assigned expert\n", "        output = torch.zeros_like(x)\n", "        \n", "        for expert_idx in range(self.n_experts):\n", "            # Get mask for this expert\n", "            mask = (expert_indices == expert_idx)  # (B, T)\n", "            \n", "            if mask.any():\n", "                # Get tokens for this expert\n", "                expert_input = x[mask]  # (num_tokens, C)\n", "                expert_output = self.experts[expert_idx](expert_input)\n", "                \n", "                # Scale by routing probability\n", "                expert_output = expert_output * top_probs[mask].unsqueeze(-1)\n", "                \n", "                # Put back\n", "                output[mask] = expert_output\n", "        \n", "        # Compute auxiliary loss\n", "        aux_loss = load_balancing_loss(router_probs, expert_mask)\n", "        \n", "        return output, aux_loss, expert_indices\n", "\n", "class SwitchBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, d_ff, n_experts=4, dropout=0.1):\n", "        super().__init__()\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.switch_ffn = SwitchFFN(d_model, d_ff, n_experts, dropout)\n", "    \n", "    def forward(self, x, mask=None):\n", "        # Self-attention\n", "        x_norm = self.norm1(x)\n", "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask)\n", "        x = x + attn_out\n", "        \n", "        # Switch FFN\n", "        ffn_out, aux_loss, expert_indices = self.switch_ffn(self.norm2(x))\n", "        x = x + ffn_out\n", "        \n", "        return x, aux_loss, expert_indices\n", "\n", "class SwitchTransformer(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, d_ff=256, n_experts=4, max_len=512, dropout=0.1):\n", "        super().__init__()\n", "        self.embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = nn.Embedding(max_len, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        self.layers = nn.ModuleList([SwitchBlock(d_model, n_heads, d_ff, n_experts, dropout) for _ in range(n_layers)])\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size)\n", "    \n", "    def forward(self, x):\n", "        B, T = x.shape\n", "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n", "        \n", "        # Causal mask\n", "        mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n", "        \n", "        x = self.dropout(self.embed(x) + self.pos_embed(pos))\n", "        \n", "        total_aux_loss = 0\n", "        expert_usage = []\n", "        \n", "        for layer in self.layers:\n", "            x, aux_loss, expert_indices = layer(x, mask)\n", "            total_aux_loss += aux_loss\n", "            expert_usage.append(expert_indices)\n", "        \n", "        return self.head(self.norm(x)), total_aux_loss / len(self.layers), expert_usage\n", "\n", "model = SwitchTransformer(vocab_size=1000, d_model=64, n_heads=4, n_layers=2, d_ff=128, n_experts=4).to(device)\n", "print(f'Switch Transformer Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training Switch Transformer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Dataset\n", "text = 'the quick brown fox jumps over the lazy dog ' * 300\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n", "\n", "# Training\n", "seq_len = 64\n", "model = SwitchTransformer(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, d_ff=128, n_experts=4, max_len=seq_len).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "\n", "losses = []\n", "aux_losses = []\n", "n_steps = 300\n", "aux_weight = 0.01  # Weight for load balancing loss\n", "\n", "print('Training Switch Transformer with MoE...')\n", "for step in range(n_steps):\n", "    idx = torch.randint(0, len(data) - seq_len - 1, (16,))\n", "    x = torch.stack([data[i:i+seq_len] for i in idx]).to(device)\n", "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx]).to(device)\n", "    \n", "    optimizer.zero_grad()\n", "    logits, aux_loss, _ = model(x)\n", "    \n", "    main_loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "    total_loss = main_loss + aux_weight * aux_loss\n", "    \n", "    total_loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(main_loss.item())\n", "    aux_losses.append(aux_loss.item())\n", "    \n", "    if (step + 1) % 50 == 0:\n", "        print(f'Step {step+1}: Main Loss = {main_loss.item():.4f}, Aux Loss = {aux_loss.item():.4f}')\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n", "axes[0].plot(losses)\n", "axes[0].set_xlabel('Step')\n", "axes[0].set_ylabel('Main Loss')\n", "axes[0].set_title('Language Modeling Loss')\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "axes[1].plot(aux_losses, color='orange')\n", "axes[1].set_xlabel('Step')\n", "axes[1].set_ylabel('Load Balancing Loss')\n", "axes[1].set_title('Auxiliary Loss (Expert Balance)')\n", "axes[1].grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Analyze expert utilization\n", "model.eval()\n", "test_input = torch.randint(0, vocab_size, (4, 64)).to(device)\n", "\n", "with torch.no_grad():\n", "    _, _, expert_usage = model(test_input)\n", "\n", "# Count expert usage across layers\n", "fig, axes = plt.subplots(1, len(expert_usage), figsize=(12, 4))\n", "\n", "for layer_idx, expert_indices in enumerate(expert_usage):\n", "    counts = torch.bincount(expert_indices.flatten(), minlength=4).cpu()\n", "    axes[layer_idx].bar(range(4), counts, color=plt.cm.Set3(range(4)))\n", "    axes[layer_idx].set_xlabel('Expert')\n", "    axes[layer_idx].set_ylabel('Tokens')\n", "    axes[layer_idx].set_title(f'Layer {layer_idx + 1}')\n", "    axes[layer_idx].set_xticks(range(4))\n", "\n", "plt.suptitle('Expert Utilization by Layer')\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. MoE: Multiple FFN experts, tokens routed dynamically')\n", "print('2. Switch (top-1): Each token goes to exactly 1 expert')\n", "print('3. Scales to trillions of parameters with constant FLOPs')\n", "print('4. Load balancing loss prevents expert collapse')\n", "print('5. Used in Google Switch-C (1.6T params), GLaM, Mixtral')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
