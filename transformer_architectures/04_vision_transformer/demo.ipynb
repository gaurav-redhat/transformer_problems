{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# üëÅÔ∏è Vision Transformer (ViT)\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/04_vision_transformer/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovation\n", "- **Image patches as tokens**: Split image into 16√ó16 patches\n", "- **[CLS] token**: For classification\n", "- **Pure transformer**: No convolutions needed!"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch torchvision matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torchvision\n", "import torchvision.transforms as transforms\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## ViT Architecture"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PatchEmbedding(nn.Module):\n", "    \"\"\"Split image into patches and embed them.\"\"\"\n", "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=128):\n", "        super().__init__()\n", "        self.img_size = img_size\n", "        self.patch_size = patch_size\n", "        self.n_patches = (img_size // patch_size) ** 2\n", "        \n", "        # Conv2d acts as patch embedding\n", "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n", "    \n", "    def forward(self, x):\n", "        # x: (B, C, H, W)\n", "        x = self.proj(x)  # (B, embed_dim, n_patches_h, n_patches_w)\n", "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n", "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n", "        return x\n", "\n", "class ViTAttention(nn.Module):\n", "    def __init__(self, embed_dim, n_heads, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.head_dim = embed_dim // n_heads\n", "        self.scale = self.head_dim ** -0.5\n", "        \n", "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n", "        self.proj = nn.Linear(embed_dim, embed_dim)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, x):\n", "        B, N, C = x.shape\n", "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n", "        Q, K, V = qkv[0], qkv[1], qkv[2]\n", "        \n", "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n", "        attn = attn.softmax(dim=-1)\n", "        attn = self.dropout(attn)\n", "        \n", "        x = (attn @ V).transpose(1, 2).reshape(B, N, C)\n", "        return self.proj(x), attn\n", "\n", "class ViTBlock(nn.Module):\n", "    def __init__(self, embed_dim, n_heads, mlp_ratio=4, dropout=0.1):\n", "        super().__init__()\n", "        self.norm1 = nn.LayerNorm(embed_dim)\n", "        self.attn = ViTAttention(embed_dim, n_heads, dropout)\n", "        self.norm2 = nn.LayerNorm(embed_dim)\n", "        self.mlp = nn.Sequential(\n", "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n", "            nn.GELU(),\n", "            nn.Dropout(dropout),\n", "            nn.Linear(embed_dim * mlp_ratio, embed_dim),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        attn_out, attn_weights = self.attn(self.norm1(x))\n", "        x = x + attn_out\n", "        x = x + self.mlp(self.norm2(x))\n", "        return x, attn_weights\n", "\n", "class ViT(nn.Module):\n", "    def __init__(self, img_size=32, patch_size=4, in_channels=3, n_classes=10,\n", "                 embed_dim=128, n_heads=4, n_layers=4, dropout=0.1):\n", "        super().__init__()\n", "        \n", "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n", "        n_patches = self.patch_embed.n_patches\n", "        \n", "        # [CLS] token and position embeddings\n", "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n", "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        # Transformer blocks\n", "        self.blocks = nn.ModuleList([ViTBlock(embed_dim, n_heads, dropout=dropout) for _ in range(n_layers)])\n", "        \n", "        # Classification head\n", "        self.norm = nn.LayerNorm(embed_dim)\n", "        self.head = nn.Linear(embed_dim, n_classes)\n", "        \n", "        self._init_weights()\n", "    \n", "    def _init_weights(self):\n", "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n", "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n", "    \n", "    def forward(self, x):\n", "        B = x.shape[0]\n", "        \n", "        # Patch embedding\n", "        x = self.patch_embed(x)\n", "        \n", "        # Add [CLS] token\n", "        cls_tokens = self.cls_token.expand(B, -1, -1)\n", "        x = torch.cat([cls_tokens, x], dim=1)\n", "        \n", "        # Add position embedding\n", "        x = x + self.pos_embed\n", "        x = self.dropout(x)\n", "        \n", "        # Transformer blocks\n", "        attn_weights = []\n", "        for block in self.blocks:\n", "            x, attn = block(x)\n", "            attn_weights.append(attn)\n", "        \n", "        # Classification\n", "        x = self.norm(x)\n", "        cls_output = x[:, 0]  # [CLS] token\n", "        return self.head(cls_output), attn_weights\n", "\n", "model = ViT(img_size=32, patch_size=4, n_classes=10, embed_dim=64, n_heads=4, n_layers=3).to(device)\n", "print(f'ViT Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Visualize Patch Embedding"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize how image is split into patches\n", "def visualize_patches(img, patch_size=4):\n", "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n", "    \n", "    # Original image\n", "    axes[0].imshow(img.permute(1, 2, 0))\n", "    axes[0].set_title('Original Image')\n", "    axes[0].axis('off')\n", "    \n", "    # Image with patch grid\n", "    axes[1].imshow(img.permute(1, 2, 0))\n", "    h, w = img.shape[1], img.shape[2]\n", "    for i in range(0, h, patch_size):\n", "        axes[1].axhline(y=i, color='r', linewidth=1)\n", "    for j in range(0, w, patch_size):\n", "        axes[1].axvline(x=j, color='r', linewidth=1)\n", "    axes[1].set_title(f'Patches ({h//patch_size}√ó{w//patch_size} = {(h//patch_size)**2} patches)')\n", "    axes[1].axis('off')\n", "    \n", "    # Patches as sequence\n", "    n_patches = (h // patch_size) ** 2\n", "    patch_sequence = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n", "    patch_sequence = patch_sequence.contiguous().view(3, -1, patch_size, patch_size)\n", "    \n", "    for i in range(min(16, n_patches)):\n", "        ax = fig.add_subplot(3, 20, 41 + i)\n", "        ax.imshow(patch_sequence[:, i].permute(1, 2, 0))\n", "        ax.axis('off')\n", "    \n", "    axes[2].axis('off')\n", "    axes[2].set_title('First 16 Patches as Sequence')\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "# Create sample image\n", "sample_img = torch.rand(3, 32, 32)\n", "visualize_patches(sample_img, patch_size=4)"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training on CIFAR-10"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load CIFAR-10\n", "transform = transforms.Compose([\n", "    transforms.ToTensor(),\n", "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n", "])\n", "\n", "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n", "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n", "\n", "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n", "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n", "\n", "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n", "print(f'Training samples: {len(trainset)}')\n", "print(f'Test samples: {len(testset)}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training\n", "model = ViT(img_size=32, patch_size=4, n_classes=10, embed_dim=64, n_heads=4, n_layers=4).to(device)\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n", "criterion = nn.CrossEntropyLoss()\n", "\n", "n_epochs = 5\n", "losses = []\n", "accuracies = []\n", "\n", "print('Training ViT on CIFAR-10...')\n", "for epoch in range(n_epochs):\n", "    model.train()\n", "    running_loss = 0.0\n", "    \n", "    for i, (images, labels) in enumerate(trainloader):\n", "        images, labels = images.to(device), labels.to(device)\n", "        \n", "        optimizer.zero_grad()\n", "        outputs, _ = model(images)\n", "        loss = criterion(outputs, labels)\n", "        loss.backward()\n", "        optimizer.step()\n", "        \n", "        running_loss += loss.item()\n", "        \n", "        if (i + 1) % 200 == 0:\n", "            print(f'Epoch {epoch+1}, Step {i+1}: Loss = {running_loss/200:.4f}')\n", "            losses.append(running_loss/200)\n", "            running_loss = 0.0\n", "    \n", "    # Evaluate\n", "    model.eval()\n", "    correct = 0\n", "    total = 0\n", "    with torch.no_grad():\n", "        for images, labels in testloader:\n", "            images, labels = images.to(device), labels.to(device)\n", "            outputs, _ = model(images)\n", "            _, predicted = outputs.max(1)\n", "            total += labels.size(0)\n", "            correct += predicted.eq(labels).sum().item()\n", "    \n", "    acc = 100 * correct / total\n", "    accuracies.append(acc)\n", "    print(f'Epoch {epoch+1} Test Accuracy: {acc:.2f}%')\n", "\n", "# Plot\n", "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n", "axes[0].plot(losses)\n", "axes[0].set_xlabel('Step (√ó200)')\n", "axes[0].set_ylabel('Loss')\n", "axes[0].set_title('Training Loss')\n", "axes[0].grid(True, alpha=0.3)\n", "\n", "axes[1].plot(accuracies, 'o-')\n", "axes[1].set_xlabel('Epoch')\n", "axes[1].set_ylabel('Accuracy (%)')\n", "axes[1].set_title('Test Accuracy')\n", "axes[1].grid(True, alpha=0.3)\n", "plt.tight_layout()\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize attention\n", "model.eval()\n", "images, labels = next(iter(testloader))\n", "img = images[0:1].to(device)\n", "\n", "with torch.no_grad():\n", "    _, attn_weights = model(img)\n", "\n", "# Visualize attention from [CLS] to patches\n", "attn = attn_weights[-1][0, 0, 0, 1:].reshape(8, 8).cpu()  # Last layer, head 0, from CLS\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n", "\n", "# Original image\n", "axes[0].imshow(images[0].permute(1, 2, 0) * 0.5 + 0.5)\n", "axes[0].set_title(f'Input Image: {classes[labels[0]]}')\n", "axes[0].axis('off')\n", "\n", "# Attention map\n", "axes[1].imshow(attn, cmap='hot')\n", "axes[1].set_title('Attention from [CLS] Token')\n", "axes[1].axis('off')\n", "\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('\\nüéØ Key Takeaways:')\n", "print('1. ViT treats image patches as tokens')\n", "print('2. [CLS] token aggregates information for classification')\n", "print('3. No convolutions - pure transformer')\n", "print('4. Needs large data or pretraining for best results')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
