{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ“œ Longformer: Long Document Transformer\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/transformer/transformer_architectures/09_longformer/demo.ipynb)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### Key Innovation\n", "- **Sliding Window Attention**: Local context efficiently\n", "- **Dilated Sliding Window**: Larger receptive field\n", "- **Global Attention**: Selected tokens attend to all"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Longformer Attention Patterns"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_longformer_patterns(seq_len, window_size=4, dilation=2, global_indices=[0]):\n", "    \"\"\"Create Longformer attention patterns.\"\"\"\n", "    patterns = {}\n", "    \n", "    # 1. Sliding window attention\n", "    sliding = torch.zeros(seq_len, seq_len)\n", "    for i in range(seq_len):\n", "        start = max(0, i - window_size // 2)\n", "        end = min(seq_len, i + window_size // 2 + 1)\n", "        sliding[i, start:end] = 1\n", "    patterns['Sliding Window'] = sliding\n", "    \n", "    # 2. Dilated sliding window\n", "    dilated = torch.zeros(seq_len, seq_len)\n", "    for i in range(seq_len):\n", "        for offset in range(-window_size // 2, window_size // 2 + 1):\n", "            j = i + offset * dilation\n", "            if 0 <= j < seq_len:\n", "                dilated[i, j] = 1\n", "    patterns['Dilated Window'] = dilated\n", "    \n", "    # 3. Global attention (specific tokens attend to everything)\n", "    global_attn = torch.zeros(seq_len, seq_len)\n", "    for idx in global_indices:\n", "        global_attn[idx, :] = 1  # Global token attends to all\n", "        global_attn[:, idx] = 1  # All attend to global token\n", "    patterns['Global Tokens'] = global_attn\n", "    \n", "    # 4. Combined Longformer pattern\n", "    combined = (sliding + global_attn).clamp(0, 1)\n", "    patterns['Longformer Combined'] = combined\n", "    \n", "    return patterns\n", "\n", "# Visualize patterns\n", "seq_len = 32\n", "patterns = create_longformer_patterns(seq_len, window_size=6, global_indices=[0, 15, 31])\n", "\n", "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n", "for ax, (name, pattern) in zip(axes, patterns.items()):\n", "    ax.imshow(pattern, cmap='Blues')\n", "    ax.set_title(f'{name}\\n{pattern.sum().item():.0f}/{seq_len**2} = {pattern.mean().item()*100:.1f}%')\n", "    ax.set_xlabel('Key')\n", "    ax.set_ylabel('Query')\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "print('Sliding window: Each token attends to nearby tokens')\n", "print('Global tokens: [CLS], special positions attend everywhere')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Complexity Analysis"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def complexity_comparison():\n", "    seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n", "    window_size = 256\n", "    n_global = 4\n", "    \n", "    # Full attention: O(NÂ²)\n", "    full = [n**2 for n in seq_lengths]\n", "    \n", "    # Longformer: O(N Ã— window) + O(N Ã— n_global)\n", "    longformer = [n * window_size + n * n_global for n in seq_lengths]\n", "    \n", "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "    \n", "    # Absolute\n", "    ax = axes[0]\n", "    ax.plot(seq_lengths, full, 'ro-', label='Full Attention O(NÂ²)', linewidth=2)\n", "    ax.plot(seq_lengths, longformer, 'go-', label='Longformer O(NÂ·w)', linewidth=2)\n", "    ax.set_xlabel('Sequence Length')\n", "    ax.set_ylabel('Operations')\n", "    ax.set_title('Absolute Complexity')\n", "    ax.legend()\n", "    ax.set_yscale('log')\n", "    ax.grid(True, alpha=0.3)\n", "    \n", "    # Speedup\n", "    ax = axes[1]\n", "    speedup = [f/l for f, l in zip(full, longformer)]\n", "    ax.bar(range(len(seq_lengths)), speedup, color='green', alpha=0.7)\n", "    ax.set_xticks(range(len(seq_lengths)))\n", "    ax.set_xticklabels(seq_lengths)\n", "    ax.set_xlabel('Sequence Length')\n", "    ax.set_ylabel('Speedup (Ã—)')\n", "    ax.set_title('Longformer Speedup over Full Attention')\n", "    for i, s in enumerate(speedup):\n", "        ax.text(i, s + 0.5, f'{s:.0f}Ã—', ha='center')\n", "    ax.grid(True, alpha=0.3)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "    print(f'\\nAt seq_len=16384:')\n", "    print(f'  Full: {full[-1]:,} ops')\n", "    print(f'  Longformer: {longformer[-1]:,} ops')\n", "    print(f'  Speedup: {speedup[-1]:.0f}Ã—')\n", "\n", "complexity_comparison()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Longformer Implementation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LongformerAttention(nn.Module):\n", "    \"\"\"Sliding window + global attention.\"\"\"\n", "    def __init__(self, d_model, n_heads, window_size=64, dropout=0.1):\n", "        super().__init__()\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        self.scale = self.d_k ** -0.5\n", "        self.window_size = window_size\n", "        \n", "        self.W_qkv = nn.Linear(d_model, 3 * d_model)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def _create_window_mask(self, seq_len):\n", "        \"\"\"Create sliding window attention mask.\"\"\"\n", "        mask = torch.zeros(seq_len, seq_len)\n", "        half_window = self.window_size // 2\n", "        \n", "        for i in range(seq_len):\n", "            start = max(0, i - half_window)\n", "            end = min(seq_len, i + half_window + 1)\n", "            mask[i, start:end] = 1\n", "        \n", "        return mask\n", "    \n", "    def forward(self, x, global_mask=None):\n", "        B, T, C = x.shape\n", "        \n", "        # QKV projection\n", "        qkv = self.W_qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n", "        Q, K, V = qkv[0], qkv[1], qkv[2]\n", "        \n", "        # Compute attention\n", "        attn = (Q @ K.transpose(-2, -1)) * self.scale\n", "        \n", "        # Apply sliding window mask\n", "        window_mask = self._create_window_mask(T).to(x.device)\n", "        \n", "        # Add global attention if specified\n", "        if global_mask is not None:\n", "            # Global tokens can attend to everything\n", "            combined_mask = window_mask.clone()\n", "            combined_mask[global_mask, :] = 1  # Global queries\n", "            combined_mask[:, global_mask] = 1  # All attend to global keys\n", "        else:\n", "            combined_mask = window_mask\n", "        \n", "        # Apply causal constraint\n", "        causal_mask = torch.tril(torch.ones(T, T, device=x.device))\n", "        combined_mask = combined_mask * causal_mask\n", "        \n", "        attn = attn.masked_fill(combined_mask.unsqueeze(0).unsqueeze(0) == 0, float('-inf'))\n", "        attn = F.softmax(attn, dim=-1)\n", "        attn = self.dropout(attn)\n", "        \n", "        out = (attn @ V).transpose(1, 2).reshape(B, T, C)\n", "        return self.W_o(out), attn\n", "\n", "class LongformerBlock(nn.Module):\n", "    def __init__(self, d_model, n_heads, window_size=64, dropout=0.1):\n", "        super().__init__()\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.attn = LongformerAttention(d_model, n_heads, window_size, dropout)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.ff = nn.Sequential(\n", "            nn.Linear(d_model, 4 * d_model),\n", "            nn.GELU(),\n", "            nn.Linear(4 * d_model, d_model),\n", "            nn.Dropout(dropout)\n", "        )\n", "    \n", "    def forward(self, x, global_mask=None):\n", "        attn_out, attn_weights = self.attn(self.norm1(x), global_mask)\n", "        x = x + attn_out\n", "        x = x + self.ff(self.norm2(x))\n", "        return x, attn_weights\n", "\n", "class Longformer(nn.Module):\n", "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, window_size=64, max_len=512, dropout=0.1):\n", "        super().__init__()\n", "        self.embed = nn.Embedding(vocab_size, d_model)\n", "        self.pos_embed = nn.Embedding(max_len, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        self.layers = nn.ModuleList([LongformerBlock(d_model, n_heads, window_size, dropout) for _ in range(n_layers)])\n", "        self.norm = nn.LayerNorm(d_model)\n", "        self.head = nn.Linear(d_model, vocab_size)\n", "    \n", "    def forward(self, x, global_mask=None):\n", "        B, T = x.shape\n", "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n", "        \n", "        x = self.dropout(self.embed(x) + self.pos_embed(pos))\n", "        \n", "        attn_weights = []\n", "        for layer in self.layers:\n", "            x, attn = layer(x, global_mask)\n", "            attn_weights.append(attn)\n", "        \n", "        return self.head(self.norm(x)), attn_weights\n", "\n", "model = Longformer(vocab_size=1000, d_model=64, n_heads=4, n_layers=2, window_size=16).to(device)\n", "print(f'Longformer Parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Training Longformer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Dataset\n", "text = 'the quick brown fox jumps over the lazy dog and the cat sleeps on the warm mat ' * 200\n", "chars = sorted(list(set(text)))\n", "vocab_size = len(chars)\n", "char_to_idx = {c: i for i, c in enumerate(chars)}\n", "data = torch.tensor([char_to_idx[c] for c in text], dtype=torch.long)\n", "\n", "# Training with global tokens\n", "seq_len = 128\n", "model = Longformer(vocab_size=vocab_size, d_model=64, n_heads=4, n_layers=2, window_size=16, max_len=seq_len).to(device)\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "\n", "# Global mask: first token and every 32nd token\n", "global_indices = torch.tensor([0, 32, 64, 96]).to(device)\n", "\n", "losses = []\n", "n_steps = 300\n", "\n", "print('Training Longformer with sliding window + global attention...')\n", "for step in range(n_steps):\n", "    idx = torch.randint(0, len(data) - seq_len - 1, (16,))\n", "    x = torch.stack([data[i:i+seq_len] for i in idx]).to(device)\n", "    y = torch.stack([data[i+1:i+seq_len+1] for i in idx]).to(device)\n", "    \n", "    optimizer.zero_grad()\n", "    logits, _ = model(x, global_mask=global_indices)\n", "    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n", "    loss.backward()\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    if (step + 1) % 50 == 0:\n", "        print(f'Step {step+1}: Loss = {loss.item():.4f}')\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Step')\n", "plt.ylabel('Loss')\n", "plt.title('Longformer Training')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize attention pattern\n", "model.eval()\n", "test_input = torch.randint(0, vocab_size, (1, 64)).to(device)\n", "global_mask = torch.tensor([0, 31, 63]).to(device)\n", "\n", "with torch.no_grad():\n", "    _, attn_weights = model(test_input, global_mask=global_mask)\n", "\n", "attn = attn_weights[-1][0, 0].cpu()\n", "\n", "plt.figure(figsize=(10, 8))\n", "plt.imshow(attn, cmap='Blues')\n", "plt.xlabel('Key Position')\n", "plt.ylabel('Query Position')\n", "plt.title('Longformer Attention Pattern\\n(Global tokens: 0, 31, 63)')\n", "plt.colorbar()\n", "\n", "# Mark global positions\n", "for g in [0, 31, 63]:\n", "    plt.axhline(y=g, color='red', linestyle='--', alpha=0.5, linewidth=0.5)\n", "    plt.axvline(x=g, color='red', linestyle='--', alpha=0.5, linewidth=0.5)\n", "\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. Sliding window: Local attention O(N Ã— window)')\n", "print('2. Global tokens: Selected positions attend everywhere')\n", "print('3. Ideal for long documents: [CLS] gets global view')\n", "print('4. Used in document QA, summarization, classification')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
