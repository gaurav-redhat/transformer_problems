<p align="center">
  <img src="https://img.shields.io/badge/Architecture-Longformer-607D8B?style=for-the-badge" alt="Longformer"/>
  <img src="https://img.shields.io/badge/Complexity-O(N)-green?style=for-the-badge" alt="Complexity"/>
  <img src="https://img.shields.io/badge/Method-Sliding_Window-blue?style=for-the-badge" alt="Method"/>
</p>

<h1 align="center">09. Longformer</h1>

<p align="center">
  <a href="../README.md">â† Back</a> â€¢
  <a href="../08_reformer/README.md">â† Prev</a> â€¢
  <a href="../10_switch_transformer/README.md">Next: Switch â†’</a>
</p>

<p align="center">
  <a href="https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/09_longformer/demo.ipynb">
    <img src="https://img.shields.io/badge/â–¶_Open_in_Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white" alt="Open In Colab"/>
  </a>
</p>

---

<p align="center">
  <img src="architecture.png" alt="Architecture" width="90%"/>
</p>

---

## ğŸ’¡ The Idea

Longformer is probably the most **practical** efficient attention method.

> *Local attention for most tokens + Global attention for special tokens.*

No random features. No LSH. Just a sensible pattern that works.

---

## ğŸ¯ The Insight

Think about how you read:
- Most words only matter in context of **nearby words**
- But some tokens (title, question) need to see **everything**

<table>
<tr>
<td align="center" width="50%">

### ğŸ”² Local (Sliding Window)
Each token attends to w neighbors

```
Token i â†’ [i-w/2, ..., i, ..., i+w/2]
```

**Complexity: O(N Ã— w)**

</td>
<td align="center" width="50%">

### ğŸŒ Global
Special tokens see everything

```
[CLS] â†’ [all tokens]
[all tokens] â†’ [CLS]
```

**For: classification, questions**

</td>
</tr>
</table>

---

## ğŸ“Š The Speedup

```
N = 4096, w = 512

Standard:  4096Â² = 16M operations
Longformer: 4096 Ã— 512 = 2M operations

Speedup: 8Ã—
```

And it's **exact** â€” no approximation!

---

## ğŸ¯ Task-Specific Globals

| Task | Global Tokens |
|------|:-------------:|
| Classification | `[CLS]` |
| Question Answering | Question tokens |
| Summarization | `[CLS]` + paragraph starts |
| NER | None (all local) |

---

## ğŸ“ The Math

```
A_ij = 1 if:
  - |i - j| â‰¤ w/2       (within window)
  - OR i is global
  - OR j is global

Complexity: O(N Ã— (w + g))
```

Where g = number of global tokens (usually tiny).

---

## ğŸ†š Longformer vs BERT

| Model | Max Length | Memory (4K) |
|-------|:----------:|:-----------:|
| BERT-base | 512 | OOM âŒ |
| Longformer-base | 4096 | ~3GB âœ… |
| Longformer-large | 4096 | ~8GB âœ… |

---

## ğŸ’» Code

```python
def sliding_window_mask(seq_len, window_size):
    mask = torch.zeros(seq_len, seq_len)
    half_w = window_size // 2
    
    for i in range(seq_len):
        start = max(0, i - half_w)
        end = min(seq_len, i + half_w + 1)
        mask[i, start:end] = 1
    
    return mask

def add_global(mask, global_indices):
    for idx in global_indices:
        mask[idx, :] = 1  # Global sees all
        mask[:, idx] = 1  # All see global
    return mask
```

---

## âœ… When to Use

| âœ… Good For | âŒ Not Needed |
|------------|--------------|
| Long document classification | Short texts (< 512) |
| Long-form QA | Tasks needing full attention |
| Summarization | |
| Legal/medical docs | |

---

## ğŸ†š vs Other Methods

| Method | Complexity | Exact? | Simple? |
|--------|:----------:|:------:|:-------:|
| Standard | O(NÂ²) | âœ… | âœ… |
| Sparse | O(NâˆšN) | âŒ | âš ï¸ |
| Performer | O(N) | âŒ | âŒ |
| Reformer | O(N log N) | âŒ | âŒ |
| **Longformer** | **O(N Ã— w)** | **âœ…** | **âœ…** |

> ğŸ’¡ *Longformer wins on simplicity â€” just masked attention with a sensible pattern.*

---

## ğŸ“š Papers

| Paper | Year |
|-------|:----:|
| [Longformer](https://arxiv.org/abs/2004.05150) | 2020 |
| [BigBird](https://arxiv.org/abs/2007.14062) | 2020 |

---

<p align="center">
  <a href="https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/09_longformer/demo.ipynb">
    <img src="https://img.shields.io/badge/â–¶_Train_It_Yourself-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white" alt="Open In Colab"/>
  </a>
</p>

<p align="center">
  <sub>Implement sliding window â€¢ Add global attention â€¢ Compare vs full attention</sub>
</p>
