{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# ðŸ”„ Vanilla Transformer: Complete Implementation & Training\n", "\n", "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/transformer_architectures/01_vanilla_transformer/demo.ipynb)\n", "\n", "## \"Attention Is All You Need\" (Vaswani et al., 2017)\n", "\n", "![Architecture](architecture.png)\n", "\n", "### What You'll Learn\n", "- Multi-Head Self-Attention from scratch\n", "- Positional Encoding\n", "- Encoder-Decoder architecture\n", "- Training on machine translation task"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch matplotlib numpy -q\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import math\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "\n", "torch.manual_seed(42)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f'Using device: {device}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1: Positional Encoding\n", "\n", "Since transformers have no recurrence, we need to inject position information:\n", "\n", "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n", "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PositionalEncoding(nn.Module):\n", "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n", "        super().__init__()\n", "        self.dropout = nn.Dropout(dropout)\n", "        \n", "        # Create positional encoding matrix\n", "        pe = torch.zeros(max_len, d_model)\n", "        position = torch.arange(0, max_len).unsqueeze(1).float()\n", "        \n", "        # Compute div_term: 10000^(2i/d_model)\n", "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n", "                            (-math.log(10000.0) / d_model))\n", "        \n", "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n", "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n", "        \n", "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n", "        self.register_buffer('pe', pe)\n", "    \n", "    def forward(self, x):\n", "        # x: (batch, seq_len, d_model)\n", "        x = x + self.pe[:, :x.size(1), :]\n", "        return self.dropout(x)\n", "\n", "# Visualize positional encoding\n", "pe = PositionalEncoding(128, 100, dropout=0)\n", "pe_values = pe.pe[0, :50, :64].numpy()\n", "\n", "plt.figure(figsize=(12, 4))\n", "plt.imshow(pe_values.T, cmap='RdBu', aspect='auto')\n", "plt.xlabel('Position')\n", "plt.ylabel('Dimension')\n", "plt.title('Positional Encoding Visualization')\n", "plt.colorbar()\n", "plt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 2: Multi-Head Attention\n", "\n", "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n", "\n", "Multi-head allows attending to different positions:"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MultiHeadAttention(nn.Module):\n", "    def __init__(self, d_model, n_heads, dropout=0.1):\n", "        super().__init__()\n", "        assert d_model % n_heads == 0\n", "        \n", "        self.d_model = d_model\n", "        self.n_heads = n_heads\n", "        self.d_k = d_model // n_heads\n", "        \n", "        # Linear projections for Q, K, V\n", "        self.W_q = nn.Linear(d_model, d_model)\n", "        self.W_k = nn.Linear(d_model, d_model)\n", "        self.W_v = nn.Linear(d_model, d_model)\n", "        self.W_o = nn.Linear(d_model, d_model)\n", "        \n", "        self.dropout = nn.Dropout(dropout)\n", "        self.scale = math.sqrt(self.d_k)\n", "    \n", "    def forward(self, query, key, value, mask=None):\n", "        batch_size = query.size(0)\n", "        \n", "        # Linear projections and reshape to (batch, n_heads, seq_len, d_k)\n", "        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n", "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n", "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n", "        \n", "        # Attention scores: (batch, n_heads, seq_q, seq_k)\n", "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n", "        \n", "        if mask is not None:\n", "            scores = scores.masked_fill(mask == 0, float('-inf'))\n", "        \n", "        attn_weights = F.softmax(scores, dim=-1)\n", "        attn_weights = self.dropout(attn_weights)\n", "        \n", "        # Apply attention to values\n", "        context = torch.matmul(attn_weights, V)\n", "        \n", "        # Reshape and project\n", "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n", "        output = self.W_o(context)\n", "        \n", "        return output, attn_weights\n", "\n", "# Test attention\n", "mha = MultiHeadAttention(d_model=64, n_heads=4)\n", "x = torch.randn(2, 10, 64)  # (batch, seq_len, d_model)\n", "out, attn = mha(x, x, x)\n", "print(f'Input: {x.shape}')\n", "print(f'Output: {out.shape}')\n", "print(f'Attention weights: {attn.shape}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3: Feed-Forward Network & Encoder Layer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class FeedForward(nn.Module):\n", "    def __init__(self, d_model, d_ff, dropout=0.1):\n", "        super().__init__()\n", "        self.linear1 = nn.Linear(d_model, d_ff)\n", "        self.linear2 = nn.Linear(d_ff, d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, x):\n", "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n", "\n", "class EncoderLayer(nn.Module):\n", "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n", "        super().__init__()\n", "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n", "        self.ffn = FeedForward(d_model, d_ff, dropout)\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, x, mask=None):\n", "        # Self-attention with residual\n", "        attn_out, _ = self.self_attn(x, x, x, mask)\n", "        x = self.norm1(x + self.dropout(attn_out))\n", "        \n", "        # FFN with residual\n", "        ffn_out = self.ffn(x)\n", "        x = self.norm2(x + self.dropout(ffn_out))\n", "        \n", "        return x\n", "\n", "class DecoderLayer(nn.Module):\n", "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n", "        super().__init__()\n", "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n", "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n", "        self.ffn = FeedForward(d_model, d_ff, dropout)\n", "        self.norm1 = nn.LayerNorm(d_model)\n", "        self.norm2 = nn.LayerNorm(d_model)\n", "        self.norm3 = nn.LayerNorm(d_model)\n", "        self.dropout = nn.Dropout(dropout)\n", "    \n", "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n", "        # Masked self-attention\n", "        attn_out, _ = self.self_attn(x, x, x, tgt_mask)\n", "        x = self.norm1(x + self.dropout(attn_out))\n", "        \n", "        # Cross-attention\n", "        attn_out, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n", "        x = self.norm2(x + self.dropout(attn_out))\n", "        \n", "        # FFN\n", "        ffn_out = self.ffn(x)\n", "        x = self.norm3(x + self.dropout(ffn_out))\n", "        \n", "        return x\n", "\n", "print('Encoder and Decoder layers defined!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 4: Complete Transformer"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Transformer(nn.Module):\n", "    def __init__(self, src_vocab, tgt_vocab, d_model=256, n_heads=8, \n", "                 n_encoder_layers=3, n_decoder_layers=3, d_ff=512, dropout=0.1):\n", "        super().__init__()\n", "        \n", "        self.d_model = d_model\n", "        \n", "        # Embeddings\n", "        self.src_embed = nn.Embedding(src_vocab, d_model)\n", "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n", "        self.pos_enc = PositionalEncoding(d_model, dropout=dropout)\n", "        \n", "        # Encoder\n", "        self.encoder_layers = nn.ModuleList([\n", "            EncoderLayer(d_model, n_heads, d_ff, dropout) \n", "            for _ in range(n_encoder_layers)\n", "        ])\n", "        \n", "        # Decoder\n", "        self.decoder_layers = nn.ModuleList([\n", "            DecoderLayer(d_model, n_heads, d_ff, dropout) \n", "            for _ in range(n_decoder_layers)\n", "        ])\n", "        \n", "        # Output projection\n", "        self.output_proj = nn.Linear(d_model, tgt_vocab)\n", "        \n", "        self._init_weights()\n", "    \n", "    def _init_weights(self):\n", "        for p in self.parameters():\n", "            if p.dim() > 1:\n", "                nn.init.xavier_uniform_(p)\n", "    \n", "    def encode(self, src, src_mask=None):\n", "        x = self.src_embed(src) * math.sqrt(self.d_model)\n", "        x = self.pos_enc(x)\n", "        for layer in self.encoder_layers:\n", "            x = layer(x, src_mask)\n", "        return x\n", "    \n", "    def decode(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n", "        x = self.tgt_embed(tgt) * math.sqrt(self.d_model)\n", "        x = self.pos_enc(x)\n", "        for layer in self.decoder_layers:\n", "            x = layer(x, enc_output, src_mask, tgt_mask)\n", "        return x\n", "    \n", "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n", "        enc_output = self.encode(src, src_mask)\n", "        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n", "        return self.output_proj(dec_output)\n", "\n", "def create_causal_mask(size):\n", "    \"\"\"Create causal mask for decoder.\"\"\"\n", "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n", "    return ~mask  # True where attention is allowed\n", "\n", "# Create model\n", "model = Transformer(src_vocab=1000, tgt_vocab=1000, d_model=128, \n", "                    n_heads=4, n_encoder_layers=2, n_decoder_layers=2)\n", "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Part 5: Training on Tiny Translation Task"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create tiny synthetic dataset (number to word translation)\n", "# E.g., [1, 2, 3] -> [\"one\", \"two\", \"three\"]\n", "\n", "class TinyTranslationDataset:\n", "    def __init__(self, size=1000, max_len=8):\n", "        self.size = size\n", "        self.max_len = max_len\n", "        \n", "        # Simple vocab: numbers 0-99 -> words\n", "        self.src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n", "        self.tgt_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n", "        \n", "        # Add numbers and word representations\n", "        for i in range(100):\n", "            self.src_vocab[str(i)] = i + 3\n", "            self.tgt_vocab[f'w{i}'] = i + 3  # w0, w1, w2, ...\n", "        \n", "        self.src_vocab_size = len(self.src_vocab)\n", "        self.tgt_vocab_size = len(self.tgt_vocab)\n", "        \n", "        # Generate data\n", "        self.data = self._generate_data()\n", "    \n", "    def _generate_data(self):\n", "        data = []\n", "        for _ in range(self.size):\n", "            length = np.random.randint(2, self.max_len)\n", "            numbers = np.random.randint(0, 100, size=length)\n", "            \n", "            # Source: number tokens\n", "            src = [self.src_vocab['<sos>']] + [self.src_vocab[str(n)] for n in numbers] + [self.src_vocab['<eos>']]\n", "            # Target: word tokens (same sequence, different vocab)\n", "            tgt = [self.tgt_vocab['<sos>']] + [self.tgt_vocab[f'w{n}'] for n in numbers] + [self.tgt_vocab['<eos>']]\n", "            \n", "            data.append((src, tgt))\n", "        return data\n", "    \n", "    def get_batch(self, batch_size):\n", "        indices = np.random.choice(len(self.data), batch_size)\n", "        batch = [self.data[i] for i in indices]\n", "        \n", "        # Pad sequences\n", "        max_src = max(len(x[0]) for x in batch)\n", "        max_tgt = max(len(x[1]) for x in batch)\n", "        \n", "        src_batch = torch.zeros(batch_size, max_src, dtype=torch.long)\n", "        tgt_batch = torch.zeros(batch_size, max_tgt, dtype=torch.long)\n", "        \n", "        for i, (src, tgt) in enumerate(batch):\n", "            src_batch[i, :len(src)] = torch.tensor(src)\n", "            tgt_batch[i, :len(tgt)] = torch.tensor(tgt)\n", "        \n", "        return src_batch, tgt_batch\n", "\n", "dataset = TinyTranslationDataset(size=2000)\n", "print(f'Dataset size: {len(dataset.data)}')\n", "print(f'Source vocab: {dataset.src_vocab_size}')\n", "print(f'Target vocab: {dataset.tgt_vocab_size}')\n", "\n", "# Show example\n", "src, tgt = dataset.get_batch(1)\n", "print(f'\\nExample batch:')\n", "print(f'Source: {src[0].tolist()}')\n", "print(f'Target: {tgt[0].tolist()}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training loop\n", "model = Transformer(\n", "    src_vocab=dataset.src_vocab_size, \n", "    tgt_vocab=dataset.tgt_vocab_size,\n", "    d_model=128, n_heads=4, \n", "    n_encoder_layers=2, n_decoder_layers=2,\n", "    d_ff=256, dropout=0.1\n", ").to(device)\n", "\n", "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98))\n", "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n", "\n", "losses = []\n", "n_epochs = 100\n", "batch_size = 32\n", "\n", "print('Training Transformer...')\n", "for epoch in range(n_epochs):\n", "    model.train()\n", "    \n", "    src, tgt = dataset.get_batch(batch_size)\n", "    src, tgt = src.to(device), tgt.to(device)\n", "    \n", "    # Teacher forcing: input is tgt[:-1], target is tgt[1:]\n", "    tgt_input = tgt[:, :-1]\n", "    tgt_output = tgt[:, 1:]\n", "    \n", "    # Create causal mask\n", "    tgt_mask = create_causal_mask(tgt_input.size(1)).to(device)\n", "    \n", "    # Forward\n", "    optimizer.zero_grad()\n", "    output = model(src, tgt_input, tgt_mask=tgt_mask)\n", "    \n", "    # Loss\n", "    loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n", "    loss.backward()\n", "    \n", "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n", "    optimizer.step()\n", "    \n", "    losses.append(loss.item())\n", "    \n", "    if (epoch + 1) % 20 == 0:\n", "        print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}')\n", "\n", "# Plot training\n", "plt.figure(figsize=(10, 4))\n", "plt.plot(losses)\n", "plt.xlabel('Epoch')\n", "plt.ylabel('Loss')\n", "plt.title('Transformer Training Loss')\n", "plt.grid(True, alpha=0.3)\n", "plt.show()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Inference (greedy decoding)\n", "def greedy_decode(model, src, max_len=20):\n", "    model.eval()\n", "    with torch.no_grad():\n", "        enc_output = model.encode(src)\n", "        \n", "        # Start with <sos>\n", "        tgt = torch.tensor([[1]]).to(device)  # <sos> token\n", "        \n", "        for _ in range(max_len):\n", "            tgt_mask = create_causal_mask(tgt.size(1)).to(device)\n", "            output = model.decode(tgt, enc_output, tgt_mask=tgt_mask)\n", "            logits = model.output_proj(output[:, -1, :])\n", "            next_token = logits.argmax(dim=-1, keepdim=True)\n", "            tgt = torch.cat([tgt, next_token], dim=1)\n", "            \n", "            if next_token.item() == 2:  # <eos>\n", "                break\n", "        \n", "        return tgt[0].tolist()\n", "\n", "# Test inference\n", "print('Testing Inference:')\n", "print('='*50)\n", "\n", "for _ in range(5):\n", "    src, tgt = dataset.get_batch(1)\n", "    src = src.to(device)\n", "    \n", "    predicted = greedy_decode(model, src)\n", "    \n", "    print(f'Source:    {src[0].tolist()}')\n", "    print(f'Target:    {tgt[0].tolist()}')\n", "    print(f'Predicted: {predicted}')\n", "    print()"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize attention patterns\n", "def visualize_attention(model, src, tgt):\n", "    model.eval()\n", "    with torch.no_grad():\n", "        # Get encoder output and attention\n", "        x = model.src_embed(src) * math.sqrt(model.d_model)\n", "        x = model.pos_enc(x)\n", "        \n", "        # Get attention from first encoder layer\n", "        _, attn = model.encoder_layers[0].self_attn(x, x, x)\n", "        \n", "    return attn[0, 0].cpu().numpy()  # First batch, first head\n", "\n", "src, tgt = dataset.get_batch(1)\n", "src = src.to(device)\n", "attn = visualize_attention(model, src, tgt)\n", "\n", "plt.figure(figsize=(8, 6))\n", "plt.imshow(attn, cmap='Blues')\n", "plt.xlabel('Key Position')\n", "plt.ylabel('Query Position')\n", "plt.title('Self-Attention Pattern (Encoder, Head 0)')\n", "plt.colorbar()\n", "plt.show()\n", "\n", "print('\\nðŸŽ¯ Key Takeaways:')\n", "print('1. Transformer uses self-attention instead of recurrence')\n", "print('2. Multi-head attention captures different relationships')\n", "print('3. Encoder processes source, decoder generates target autoregressively')\n", "print('4. Positional encoding provides position information')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.0"}, "accelerator": "GPU"},
 "nbformat": 4,
 "nbformat_minor": 4
}
