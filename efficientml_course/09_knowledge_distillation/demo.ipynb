{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 9: Knowledge Distillation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/09_knowledge_distillation/demo.ipynb)\n",
        "\n",
        "Transfer knowledge from large teacher to small student model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def distillation_loss(student_logits, teacher_logits, labels, T=4.0, alpha=0.7):\n",
        "    \"\"\"\n",
        "    Knowledge Distillation Loss\n",
        "    T: Temperature (higher = softer probabilities)\n",
        "    alpha: Weight for soft vs hard targets\n",
        "    \"\"\"\n",
        "    # Soft targets from teacher\n",
        "    soft_targets = F.softmax(teacher_logits / T, dim=-1)\n",
        "    soft_loss = F.kl_div(\n",
        "        F.log_softmax(student_logits / T, dim=-1),\n",
        "        soft_targets,\n",
        "        reduction='batchmean'\n",
        "    ) * (T ** 2)\n",
        "    \n",
        "    # Hard targets (ground truth)\n",
        "    hard_loss = F.cross_entropy(student_logits, labels)\n",
        "    \n",
        "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "# Demo\n",
        "batch_size, num_classes = 4, 10\n",
        "teacher_logits = torch.randn(batch_size, num_classes) * 2  # More confident\n",
        "student_logits = torch.randn(batch_size, num_classes)\n",
        "labels = torch.randint(0, num_classes, (batch_size,))\n",
        "\n",
        "# Compare soft vs hard labels\n",
        "print(\"Why soft labels are better:\")\n",
        "print(f\"\\nHard label: {F.one_hot(labels[0], num_classes).float().numpy()}\")\n",
        "print(f\"Soft label: {F.softmax(teacher_logits[0] / 4, dim=0).detach().numpy().round(3)}\")\n",
        "\n",
        "loss = distillation_loss(student_logits, teacher_logits, labels)\n",
        "print(f\"\\nDistillation loss: {loss.item():.4f}\")\n",
        "print(\"\\nðŸŽ¯ Soft labels contain more information (class similarities)!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
