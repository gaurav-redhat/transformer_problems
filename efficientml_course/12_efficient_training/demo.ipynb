{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 12: Efficient Training\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/12_efficient_training/demo.ipynb)\n",
        "\n",
        "Mixed precision, gradient checkpointing, and memory-efficient training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "# Gradient Checkpointing Demo\n",
        "class BigBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, dim * 4)\n",
        "        self.fc2 = nn.Linear(dim * 4, dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "# Model with many layers\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, use_checkpoint=False):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([BigBlock(512) for _ in range(10)])\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint(block, x, use_reentrant=False)\n",
        "            else:\n",
        "                x = block(x)\n",
        "        return x\n",
        "\n",
        "# Compare memory usage\n",
        "x = torch.randn(32, 100, 512, requires_grad=True)\n",
        "\n",
        "# Without checkpointing\n",
        "model_normal = Model(use_checkpoint=False)\n",
        "torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
        "out = model_normal(x)\n",
        "out.sum().backward()\n",
        "\n",
        "# With checkpointing  \n",
        "model_ckpt = Model(use_checkpoint=True)\n",
        "out = model_ckpt(x)\n",
        "out.sum().backward()\n",
        "\n",
        "print(\"Gradient Checkpointing:\")\n",
        "print(\"  Without: Stores ALL intermediate activations\")\n",
        "print(\"  With: Recomputes activations during backward\")\n",
        "print(\"\\n  Trade-off: ~30% more compute, ~50% less memory\")\n",
        "print(\"\\nðŸŽ¯ Essential for training large models on limited GPU!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
