{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 7: Neural Architecture Search (Part I)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/07_neural_architecture_search_1/demo.ipynb)\n",
        "\n",
        "Implementing a simple differentiable NAS (DARTS-style).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# DARTS-style differentiable NAS\n",
        "# Instead of discrete choice, use soft weighting\n",
        "\n",
        "class MixedOp(nn.Module):\n",
        "    \"\"\"Mix of candidate operations with learnable weights\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.ops = nn.ModuleList([\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),  # Conv 3x3\n",
        "            nn.Conv2d(in_ch, out_ch, 5, padding=2),  # Conv 5x5\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),     # MaxPool\n",
        "            nn.Identity(),                            # Skip\n",
        "        ])\n",
        "        # Architecture weights (learnable)\n",
        "        self.alpha = nn.Parameter(torch.zeros(4))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        weights = F.softmax(self.alpha, dim=0)\n",
        "        out = sum(w * op(x) for w, op in zip(weights, self.ops))\n",
        "        return out\n",
        "\n",
        "# Demo\n",
        "mixed = MixedOp(32, 32)\n",
        "x = torch.randn(1, 32, 8, 8)\n",
        "y = mixed(x)\n",
        "\n",
        "print(\"Architecture weights (before training):\")\n",
        "print(f\"  Conv3x3: {F.softmax(mixed.alpha, dim=0)[0]:.3f}\")\n",
        "print(f\"  Conv5x5: {F.softmax(mixed.alpha, dim=0)[1]:.3f}\")\n",
        "print(f\"  MaxPool: {F.softmax(mixed.alpha, dim=0)[2]:.3f}\")\n",
        "print(f\"  Skip:    {F.softmax(mixed.alpha, dim=0)[3]:.3f}\")\n",
        "print(\"\\nðŸŽ¯ During training, weights learn which operation is best!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
