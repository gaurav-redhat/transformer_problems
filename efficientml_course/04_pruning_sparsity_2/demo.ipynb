{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 4: Pruning & Sparsity (Part II) - Lottery Ticket\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/04_pruning_sparsity_2/demo.ipynb)\n",
        "\n",
        "Finding winning tickets and structured sparsity patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "# Lottery Ticket Hypothesis Demo\n",
        "# \"Sparse networks can train from scratch if reset to original init\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "# Step 1: Save original initialization\n",
        "model = SimpleMLP()\n",
        "original_init = copy.deepcopy(model.state_dict())\n",
        "print(\"Step 1: Saved original random initialization\")\n",
        "\n",
        "# Step 2: \"Train\" the model (simulate with random updates)\n",
        "for p in model.parameters():\n",
        "    p.data += torch.randn_like(p) * 0.1\n",
        "print(\"Step 2: Trained the model\")\n",
        "\n",
        "# Step 3: Prune smallest 80% of weights\n",
        "def create_mask(model, sparsity=0.8):\n",
        "    masks = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            threshold = torch.quantile(param.data.abs(), sparsity)\n",
        "            masks[name] = (param.data.abs() > threshold).float()\n",
        "    return masks\n",
        "\n",
        "mask = create_mask(model, 0.8)\n",
        "print(f\"Step 3: Created mask (keeping top 20% weights)\")\n",
        "\n",
        "# Step 4: Reset to original init but keep mask = WINNING TICKET!\n",
        "winning_ticket = SimpleMLP()\n",
        "winning_ticket.load_state_dict(original_init)\n",
        "for name, param in winning_ticket.named_parameters():\n",
        "    if name in mask:\n",
        "        param.data *= mask[name]\n",
        "\n",
        "total = sum(m.numel() for m in mask.values())\n",
        "nonzero = sum(m.sum().item() for m in mask.values())\n",
        "print(f\"Step 4: Winning ticket has {nonzero:.0f}/{total} weights ({100*nonzero/total:.0f}%)\")\n",
        "print(\"\\nðŸŽ¯ This sparse network can train to same accuracy as dense!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
