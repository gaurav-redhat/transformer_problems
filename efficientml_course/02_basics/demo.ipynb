{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 2: Neural Network Basics - Compute Primitives\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/02_basics/demo.ipynb)\n",
        "\n",
        "Understanding FLOPs, memory bandwidth, and the roofline model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Calculate FLOPs for different operations\n",
        "def count_flops_linear(in_features, out_features, batch_size=1):\n",
        "    \"\"\"FLOPs for linear layer: 2 * in * out (multiply + add)\"\"\"\n",
        "    return 2 * batch_size * in_features * out_features\n",
        "\n",
        "def count_flops_conv2d(in_ch, out_ch, kernel, h, w, batch_size=1):\n",
        "    \"\"\"FLOPs for Conv2D\"\"\"\n",
        "    return 2 * batch_size * in_ch * out_ch * kernel * kernel * h * w\n",
        "\n",
        "# Compare operations\n",
        "print(\"FLOPs Comparison\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ops = {\n",
        "    \"Linear(1024, 1024)\": count_flops_linear(1024, 1024),\n",
        "    \"Linear(4096, 4096)\": count_flops_linear(4096, 4096),\n",
        "    \"Conv2d(64, 64, 3x3, 56x56)\": count_flops_conv2d(64, 64, 3, 56, 56),\n",
        "    \"Conv2d(256, 256, 3x3, 14x14)\": count_flops_conv2d(256, 256, 3, 14, 14),\n",
        "}\n",
        "\n",
        "for name, flops in ops.items():\n",
        "    print(f\"{name:35} | {flops/1e6:>8.2f} MFLOPs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arithmetic Intensity - Are we compute or memory bound?\n",
        "def arithmetic_intensity(flops, bytes_accessed):\n",
        "    \"\"\"FLOPs per byte - higher = more compute bound\"\"\"\n",
        "    return flops / bytes_accessed\n",
        "\n",
        "# Matrix multiplication A(M,K) @ B(K,N)\n",
        "M, K, N = 1024, 1024, 1024\n",
        "flops = 2 * M * K * N\n",
        "bytes_read = (M * K + K * N) * 4  # FP32\n",
        "bytes_write = M * N * 4\n",
        "total_bytes = bytes_read + bytes_write\n",
        "\n",
        "ai = arithmetic_intensity(flops, total_bytes)\n",
        "print(f\"Matrix Multiply ({M}x{K}) @ ({K}x{N})\")\n",
        "print(f\"  FLOPs: {flops/1e9:.2f} GFLOPs\")\n",
        "print(f\"  Memory: {total_bytes/1e6:.2f} MB\")\n",
        "print(f\"  Arithmetic Intensity: {ai:.1f} FLOPs/Byte\")\n",
        "print(f\"\\n  GPU Memory BW: ~2000 GB/s\")\n",
        "print(f\"  GPU Compute: ~300 TFLOPs\")\n",
        "print(f\"  Crossover point: {300e12 / 2000e9:.0f} FLOPs/Byte\")\n",
        "print(f\"\\n  This operation is: {'Compute-bound âœ“' if ai > 150 else 'Memory-bound'}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
