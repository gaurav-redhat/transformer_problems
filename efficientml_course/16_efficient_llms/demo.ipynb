{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 16: Efficient Large Language Models\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/16_efficient_llms/demo.ipynb)\n",
        "\n",
        "KV cache, speculative decoding, and LLM serving optimizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "\n",
        "# KV Cache Memory Calculator\n",
        "def kv_cache_memory_gb(model_params, layers, heads, d_head, seq_len, dtype_bytes=2):\n",
        "    \"\"\"\n",
        "    KV cache size = 2 * layers * seq_len * d_model * dtype\n",
        "    (2 for K and V)\n",
        "    \"\"\"\n",
        "    d_model = heads * d_head\n",
        "    kv_size = 2 * layers * seq_len * d_model * dtype_bytes\n",
        "    return kv_size / 1e9\n",
        "\n",
        "# LLaMA model configs\n",
        "configs = {\n",
        "    'LLaMA-7B': {'layers': 32, 'heads': 32, 'd_head': 128},\n",
        "    'LLaMA-13B': {'layers': 40, 'heads': 40, 'd_head': 128},\n",
        "    'LLaMA-70B': {'layers': 80, 'heads': 64, 'd_head': 128},\n",
        "}\n",
        "\n",
        "print(\"KV Cache Memory (FP16)\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"{'Model':<12} | {'2K ctx':>8} | {'8K ctx':>8} | {'32K ctx':>8}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for name, cfg in configs.items():\n",
        "    for seq_len in [2048, 8192, 32768]:\n",
        "        mem = kv_cache_memory_gb(0, cfg['layers'], cfg['heads'], cfg['d_head'], seq_len)\n",
        "        if seq_len == 2048:\n",
        "            print(f\"{name:<12} | {mem:>7.1f}G\", end=\"\")\n",
        "        else:\n",
        "            print(f\" | {mem:>7.1f}G\", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nðŸŽ¯ KV cache can exceed model weights for long contexts!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Speculative Decoding Demo\n",
        "import random\n",
        "\n",
        "def speculative_decode_demo():\n",
        "    \"\"\"Simulate speculative decoding speedup\"\"\"\n",
        "    \n",
        "    # Draft model: fast but less accurate\n",
        "    # Main model: slow but accurate\n",
        "    \n",
        "    draft_time = 0.01  # 10ms per token\n",
        "    main_time = 0.1    # 100ms per token\n",
        "    \n",
        "    # Generate 100 tokens\n",
        "    total_tokens = 100\n",
        "    \n",
        "    # Standard decoding\n",
        "    standard_time = total_tokens * main_time\n",
        "    \n",
        "    # Speculative decoding\n",
        "    k = 5  # Draft 5 tokens at a time\n",
        "    accept_rate = 0.7  # 70% accepted on average\n",
        "    \n",
        "    spec_time = 0\n",
        "    generated = 0\n",
        "    while generated < total_tokens:\n",
        "        # Draft k tokens\n",
        "        spec_time += k * draft_time\n",
        "        # Verify with main model (1 forward pass)\n",
        "        spec_time += main_time\n",
        "        # Accept ~70% of k tokens on average\n",
        "        accepted = int(k * accept_rate)\n",
        "        generated += max(accepted, 1)  # At least 1 token\n",
        "    \n",
        "    speedup = standard_time / spec_time\n",
        "    \n",
        "    print(\"Speculative Decoding Simulation\")\n",
        "    print(f\"  Standard: {standard_time:.1f}s for {total_tokens} tokens\")\n",
        "    print(f\"  Speculative: {spec_time:.1f}s for {total_tokens} tokens\")\n",
        "    print(f\"  Speedup: {speedup:.1f}x\")\n",
        "    print(\"\\nðŸŽ¯ Same quality, much faster generation!\")\n",
        "\n",
        "speculative_decode_demo()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
