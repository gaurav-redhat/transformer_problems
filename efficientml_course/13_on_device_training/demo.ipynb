{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 13: On-Device Training\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/13_on_device_training/demo.ipynb)\n",
        "\n",
        "TinyTL and memory-efficient backpropagation for edge devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# TinyTL: Bias-only training for memory efficiency\n",
        "class TinyTLModel(nn.Module):\n",
        "    \"\"\"Only train biases, freeze weights\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc = nn.Linear(32, 10)\n",
        "    \n",
        "    def freeze_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                param.requires_grad = False\n",
        "            # Only biases remain trainable!\n",
        "\n",
        "model = TinyTLModel()\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_before = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "model.freeze_weights()\n",
        "trainable_after = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"TinyTL: Bias-Only Training\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable (full): {trainable_before:,}\")\n",
        "print(f\"  Trainable (bias-only): {trainable_after:,}\")\n",
        "print(f\"\\n  Reduction: {trainable_before / trainable_after:.0f}x fewer gradients!\")\n",
        "print(\"\\nðŸŽ¯ Bias-only training needs ~10x less memory!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
