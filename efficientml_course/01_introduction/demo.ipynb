{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 1: Introduction to Efficient ML\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/01_introduction/demo.ipynb)\n",
        "\n",
        "This notebook demonstrates why ML efficiency matters with practical examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchvision -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Why efficiency matters - Model size comparison\n",
        "models = {\n",
        "    \"ResNet-18\": 11.7,\n",
        "    \"ResNet-50\": 25.6,\n",
        "    \"VGG-16\": 138,\n",
        "    \"GPT-2\": 1500,\n",
        "    \"GPT-3\": 175000,\n",
        "    \"LLaMA-70B\": 70000,\n",
        "}\n",
        "\n",
        "print(\"Model Sizes (Million Parameters)\")\n",
        "print(\"=\" * 40)\n",
        "for name, params in models.items():\n",
        "    bar = \"â–ˆ\" * int(min(params / 5000, 30))\n",
        "    print(f\"{name:12} | {params:>7.0f}M | {bar}\")\n",
        "    \n",
        "print(\"\\nðŸ’¡ GPT-3 is 7,000x larger than ResNet-18!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory requirements\n",
        "def calc_memory_gb(params_millions, dtype_bytes=4):\n",
        "    return params_millions * 1e6 * dtype_bytes / 1e9\n",
        "\n",
        "print(\"\\nMemory Requirements for Inference (FP32)\")\n",
        "print(\"=\" * 50)\n",
        "for name, params in models.items():\n",
        "    mem = calc_memory_gb(params)\n",
        "    status = \"âœ“ Fits 16GB GPU\" if mem < 16 else \"âœ— Too large\"\n",
        "    print(f\"{name:12} | {mem:>8.1f} GB | {status}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ This is why we need efficient ML!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
