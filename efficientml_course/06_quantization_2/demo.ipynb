{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 6: Quantization (Part II) - QAT & LLM Quantization\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/06_quantization_2/demo.ipynb)\n",
        "\n",
        "Quantization-Aware Training and 4-bit LLM inference with GPTQ/AWQ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Quantization-Aware Training (QAT) - Fake Quantization\n",
        "class FakeQuantize(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, num_bits=8):\n",
        "        qmin, qmax = 0, 2**num_bits - 1\n",
        "        scale = x.max() - x.min()\n",
        "        scale = scale / (qmax - qmin) + 1e-8\n",
        "        \n",
        "        # Quantize and dequantize\n",
        "        x_q = torch.clamp(torch.round(x / scale), qmin, qmax)\n",
        "        x_dq = x_q * scale\n",
        "        return x_dq\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Straight-Through Estimator - pass gradients unchanged\n",
        "        return grad_output, None\n",
        "\n",
        "fake_quant = FakeQuantize.apply\n",
        "\n",
        "# Demo: QAT simulates quantization during training\n",
        "x = torch.randn(10, requires_grad=True)\n",
        "y = fake_quant(x, 4)  # 4-bit quantization\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(\"Fake Quantization Demo (4-bit)\")\n",
        "print(f\"Input:  {x[:5].detach()}\")\n",
        "print(f\"Output: {y[:5].detach()}\")\n",
        "print(f\"Grad:   {x.grad[:5]} (STE passes gradients through)\")\n",
        "print(\"\\nðŸŽ¯ Model learns to be robust to quantization noise!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
