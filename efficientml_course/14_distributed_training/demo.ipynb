{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lecture 14: Distributed Training\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/efficientml-course/efficientml_course/14_distributed_training/demo.ipynb)\n",
        "\n",
        "Data parallelism, ZeRO optimization, and FSDP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "\n",
        "# ZeRO Memory Analysis\n",
        "def zero_memory_per_gpu(model_params_b, num_gpus, dtype_bytes=2):\n",
        "    \"\"\"Calculate memory per GPU for different ZeRO stages\"\"\"\n",
        "    P = model_params_b * 1e9  # Parameters\n",
        "    \n",
        "    # Memory components (FP16 training with FP32 optimizer)\n",
        "    model_mem = P * dtype_bytes\n",
        "    grad_mem = P * dtype_bytes\n",
        "    optimizer_mem = P * 4 * 2  # Adam: momentum + variance in FP32\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # No ZeRO (DDP): Everything replicated\n",
        "    results['DDP'] = (model_mem + grad_mem + optimizer_mem) / 1e9\n",
        "    \n",
        "    # ZeRO-1: Partition optimizer states\n",
        "    results['ZeRO-1'] = (model_mem + grad_mem + optimizer_mem / num_gpus) / 1e9\n",
        "    \n",
        "    # ZeRO-2: + Partition gradients\n",
        "    results['ZeRO-2'] = (model_mem + grad_mem / num_gpus + optimizer_mem / num_gpus) / 1e9\n",
        "    \n",
        "    # ZeRO-3: + Partition parameters\n",
        "    results['ZeRO-3'] = (model_mem / num_gpus + grad_mem / num_gpus + optimizer_mem / num_gpus) / 1e9\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example: 7B model on 8 GPUs\n",
        "model_size = 7  # billion params\n",
        "num_gpus = 8\n",
        "\n",
        "results = zero_memory_per_gpu(model_size, num_gpus)\n",
        "\n",
        "print(f\"Memory per GPU for {model_size}B model on {num_gpus} GPUs:\")\n",
        "print(\"=\" * 45)\n",
        "for stage, mem in results.items():\n",
        "    bar = \"â–ˆ\" * int(mem / 5)\n",
        "    fits = \"âœ“\" if mem < 80 else \"âœ—\"\n",
        "    print(f\"{stage:8} | {mem:>6.1f} GB | {fits} | {bar}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ ZeRO-3 enables training models that don't fit on single GPU!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
