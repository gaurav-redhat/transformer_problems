{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 10: Training Instability\n",
        "\n",
        "Demonstrates gradient issues and solutions like Pre-LayerNorm.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/10_training_instability/demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch matplotlib -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Post-LayerNorm (original transformer - unstable)\n",
        "class PostLNBlock(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.norm(x + self.attn(x))  # Norm AFTER residual\n",
        "\n",
        "# Pre-LayerNorm (modern - stable)\n",
        "class PreLNBlock(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.attn(self.norm(x))  # Norm BEFORE attention\n",
        "\n",
        "# Compare gradient flow\n",
        "def check_gradients(model_class, n_layers=24):\n",
        "    blocks = nn.Sequential(*[model_class(64) for _ in range(n_layers)])\n",
        "    x = torch.randn(1, 10, 64, requires_grad=True)\n",
        "    out = blocks(x)\n",
        "    out.sum().backward()\n",
        "    return x.grad.abs().mean().item()\n",
        "\n",
        "post_grad = check_gradients(PostLNBlock)\n",
        "pre_grad = check_gradients(PreLNBlock)\n",
        "\n",
        "print(\"Gradient magnitude (24 layers):\")\n",
        "print(f\"  Post-LayerNorm: {post_grad:.6f}\")\n",
        "print(f\"  Pre-LayerNorm:  {pre_grad:.6f}\")\n",
        "print(f\"\\nâœ“ Pre-LN has {pre_grad/post_grad:.1f}x stronger gradients!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
