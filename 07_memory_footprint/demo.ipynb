{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 7: High Memory Footprint\n",
        "\n",
        "Demonstrates GPU memory issues and solutions like gradient checkpointing.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/07_memory_footprint/demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Calculate memory usage for attention\n",
        "def attention_memory_mb(seq_len, d_model, n_heads, n_layers, batch_size=1, dtype_bytes=2):\n",
        "    \"\"\"Calculate memory for attention matrices\"\"\"\n",
        "    # Attention matrix: batch × heads × seq × seq\n",
        "    attn_mem = batch_size * n_heads * seq_len * seq_len * dtype_bytes\n",
        "    # KV cache: 2 × batch × layers × seq × d_model\n",
        "    kv_cache = 2 * batch_size * n_layers * seq_len * d_model * dtype_bytes\n",
        "    return (attn_mem + kv_cache) / (1024 * 1024)\n",
        "\n",
        "# Memory scaling with sequence length\n",
        "print(\"Memory Usage vs Sequence Length (7B model config)\")\n",
        "print(\"Seq Length | Attention+KV (MB)\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for seq_len in [512, 1024, 2048, 4096, 8192, 16384]:\n",
        "    mem = attention_memory_mb(seq_len, 4096, 32, 32)\n",
        "    print(f\"{seq_len:>10} | {mem:>15.1f}\")\n",
        "\n",
        "print(\"\\n⚠️ Memory grows quadratically with sequence length!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution: Gradient Checkpointing\n",
        "class CheckpointedBlock(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
        "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear2(torch.relu(self.linear1(x)))\n",
        "\n",
        "# Without checkpointing: stores all activations\n",
        "# With checkpointing: recomputes during backward pass\n",
        "\n",
        "model = CheckpointedBlock(512)\n",
        "x = torch.randn(1, 100, 512, requires_grad=True)\n",
        "\n",
        "# Normal forward\n",
        "out_normal = model(x)\n",
        "\n",
        "# With gradient checkpointing\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "out_ckpt = checkpoint(model, x, use_reentrant=False)\n",
        "\n",
        "print(\"Gradient Checkpointing:\")\n",
        "print(\"  - Normal: Store all intermediate activations\")\n",
        "print(\"  - Checkpointed: Recompute activations during backward\")\n",
        "print(\"  - Trade-off: ~30% more compute, ~50% less memory\")\n",
        "print(\"\\n✓ Essential for training large models!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
