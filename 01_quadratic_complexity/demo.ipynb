{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 1: Quadratic Complexity O(N²)\n",
        "\n",
        "This notebook demonstrates the O(N²) complexity problem in transformer attention.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/01_quadratic_complexity/demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch matplotlib numpy -q\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Problem: O(N²) Attention\n",
        "\n",
        "Standard self-attention computes scores between every pair of tokens, creating an N×N matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standard_attention(Q, K, V):\n",
        "    \"\"\"Standard O(N²) attention\"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attention_weights, V)\n",
        "\n",
        "# Measure quadratic scaling\n",
        "d_model = 64\n",
        "seq_lengths = [128, 256, 512, 1024, 2048]\n",
        "times = []\n",
        "\n",
        "print(\"Seq Length | Time (ms) | Memory (MB)\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    Q = torch.randn(1, seq_len, d_model)\n",
        "    K = torch.randn(1, seq_len, d_model)\n",
        "    V = torch.randn(1, seq_len, d_model)\n",
        "    \n",
        "    start = time.time()\n",
        "    for _ in range(10):\n",
        "        _ = standard_attention(Q, K, V)\n",
        "    elapsed = (time.time() - start) / 10 * 1000\n",
        "    times.append(elapsed)\n",
        "    \n",
        "    mem = seq_len * seq_len * 4 / (1024 * 1024)\n",
        "    print(f\"{seq_len:>10} | {elapsed:>9.2f} | {mem:>10.2f}\")\n",
        "\n",
        "print(\"\\n⚠️ Double sequence = 4x time and memory!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize quadratic growth\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(seq_lengths, times, 'b-o', linewidth=2, markersize=8)\n",
        "ax.set_xlabel('Sequence Length', fontsize=12)\n",
        "ax.set_ylabel('Time (ms)', fontsize=12)\n",
        "ax.set_title('Attention Time Grows Quadratically O(N²)', fontsize=14)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add quadratic fit line\n",
        "x = np.array(seq_lengths)\n",
        "fit = np.polyfit(x, times, 2)\n",
        "x_smooth = np.linspace(min(x), max(x), 100)\n",
        "ax.plot(x_smooth, np.polyval(fit, x_smooth), 'r--', alpha=0.5, label='Quadratic fit')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution: Linear Attention\n",
        "\n",
        "By computing K^T @ V first, we avoid creating the N×N matrix!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear_attention(Q, K, V):\n",
        "    \"\"\"Linear O(N) attention - compute K^T @ V first!\"\"\"\n",
        "    Q_prime = F.relu(Q)  # Feature map\n",
        "    K_prime = F.relu(K)\n",
        "    \n",
        "    # Key insight: compute K^T @ V first (d × d matrix, not N × N)\n",
        "    KV = torch.matmul(K_prime.transpose(-2, -1), V)  # (d, d)\n",
        "    output = torch.matmul(Q_prime, KV)  # (N, d)\n",
        "    \n",
        "    # Normalize\n",
        "    normalizer = torch.matmul(Q_prime, K_prime.sum(dim=-2, keepdim=True).transpose(-2, -1))\n",
        "    return output / (normalizer + 1e-6)\n",
        "\n",
        "# Compare scaling\n",
        "print(\"Standard vs Linear Attention:\")\n",
        "print(\"Seq Length | Standard (ms) | Linear (ms) | Speedup\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for seq_len in [512, 1024, 2048, 4096]:\n",
        "    Q = torch.randn(1, seq_len, 64)\n",
        "    K = torch.randn(1, seq_len, 64)\n",
        "    V = torch.randn(1, seq_len, 64)\n",
        "    \n",
        "    start = time.time()\n",
        "    _ = standard_attention(Q, K, V)\n",
        "    std_time = (time.time() - start) * 1000\n",
        "    \n",
        "    start = time.time()\n",
        "    _ = linear_attention(Q, K, V)\n",
        "    lin_time = (time.time() - start) * 1000\n",
        "    \n",
        "    print(f\"{seq_len:>10} | {std_time:>13.2f} | {lin_time:>11.2f} | {std_time/lin_time:>6.1f}x\")\n",
        "\n",
        "print(\"\\n✓ Linear attention scales much better for long sequences!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
