{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem 13: Large Model Size\n",
        "\n",
        "Demonstrates LoRA for efficient fine-tuning of large models.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/transformer_problems/blob/main/13_model_size/demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Original large layer\n",
        "class OriginalLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x @ self.weight.T\n",
        "\n",
        "# LoRA: Low-Rank Adaptation\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank=8):\n",
        "        super().__init__()\n",
        "        # Frozen original weights\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.01, requires_grad=False)\n",
        "        # Trainable low-rank adapters\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        original = x @ self.weight.T\n",
        "        lora = x @ self.lora_A.T @ self.lora_B.T\n",
        "        return original + lora\n",
        "\n",
        "# Compare parameters\n",
        "d_model = 4096\n",
        "original = OriginalLinear(d_model, d_model)\n",
        "lora = LoRALinear(d_model, d_model, rank=8)\n",
        "\n",
        "orig_params = sum(p.numel() for p in original.parameters() if p.requires_grad)\n",
        "lora_params = sum(p.numel() for p in lora.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Parameter Comparison (4096×4096 layer):\")\n",
        "print(f\"  Original: {orig_params:,} trainable parameters\")\n",
        "print(f\"  LoRA (r=8): {lora_params:,} trainable parameters\")\n",
        "print(f\"\\n✓ LoRA reduces trainable params by {orig_params/lora_params:.0f}x!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
